\noindent
Neural networks are powerful and flexible models capable of highly accurate predictions, transfer and multi-objective learning.
One of their biggest downsides is the amount of labelled data needed to train deep neural networks.
This work looks at three ways to tackle this label complexity problem: unsupervised and semi-supervised learning, transfer learning and active learning.
The former is described theoretically, and the usefulness of the latter two is evaluated through a series of experiments.
\\

\noindent
\textbf{Keywords:} machine learning, deep learning, neural networks, transfer learning, active learning, multi-objective learning, data-efficient learning, label-efficient learning, data engineering
