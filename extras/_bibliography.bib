@misc{murphy,
  title={Machine learning: a probabilistic perspective},
  author={Kevin, Murphy},
  year={2012},
  publisher={The MIT press}
}

@inproceedings{compressive_sensing1,
  author    = {Richard G. Baraniuk},
  title     = {Compressive sensing},
  booktitle = {42nd Annual Conference on Information Sciences and Systems, {CISS}
               2008, Princeton, NJ, USA, 19-21 March 2008},
  year      = {2008},
  url       = {https://doi.org/10.1109/CISS.2008.4558479},
  doi       = {10.1109/CISS.2008.4558479},
  timestamp = {Fri, 19 May 2017 01:25:12 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/ciss/Baraniuk08},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{compressive_sensing2,
  abstract = {This paper considers the model problem of reconstructing an object from incomplete frequency samples. Consider a discrete-time signal f&isin;C<sup>N</sup> and a randomly chosen set of frequencies &Omega;. Is it possible to reconstruct f from the partial knowledge of its Fourier coefficients on the set &Omega;? A typical result of this paper is as follows. Suppose that f is a superposition of |T| spikes f(t)=&sigma;<sub>&tau;&isin;T</sub>f(&tau;)&delta;(t-&tau;) obeying |T|&le;C<sub>M</sub>&middot;(log N)<sup>-1</sup> &middot; |&Omega;| for some constant C<sub>M</sub>>0. We do not know the locations of the spikes nor their amplitudes. Then with probability at least 1-O(N<sup>-M</sup>), f can be reconstructed exactly as the solution to the &#8467;<sub>1</sub> minimization problem. In short, exact recovery may be obtained by solving a convex optimization problem. We give numerical values for C<sub>M</sub> which depend on the desired probability of success. Our result may be interpreted as a novel kind of nonlinear sampling theorem. In effect, it says that any signal made out of |T| spikes may be recovered by convex programming from almost every set of frequencies of size O(|T|&middot;logN). Moreover, this is nearly optimal in the sense that any method succeeding with probability 1-O(N<sup>-M</sup>) would in general require a number of frequency samples at least proportional to |T|&middot;logN. The methodology extends to a variety of other situations and higher dimensions. For example, we show how one can reconstruct a piecewise constant (one- or two-dimensional) object from incomplete frequency samples - provided that the number of jumps (discontinuities) obeys the condition above - by minimizing other convex functionals such as the total variation of f.},
  added-at = {2013-05-30T04:20:11.000+0200},
  author = {Cand{\`e}s, Emmanuel J. and Romberg, Justin and Tao, Terence},
  biburl = {https://www.bibsonomy.org/bibtex/2f3e63909ebf5bdfdb7e841179e704c93/ytyoun},
  doi = {10.1109/TIT.2005.862083},
  interhash = {ff0f531efa2260488be3523d4e72a99d},
  intrahash = {f3e63909ebf5bdfdb7e841179e704c93},
  issn = {0018-9448},
  journal = {Information Theory, IEEE Transactions on},
  keywords = {compressed.sensing uncertainty},
  number = 2,
  pages = {489--509},
  timestamp = {2015-12-10T09:45:43.000+0100},
  title = {Robust Uncertainty Principles: Exact Signal Reconstruction from Highly Incomplete Frequency Information},
  volume = 52,
  year = 2006
}
@article{compressive_sensing3,
  title={Compressed sensing, sparsity, and dimensionality in neuronal information processing and data analysis},
  author={Ganguli, Surya and Sompolinsky, Haim},
  journal={Annual review of neuroscience},
  volume={35},
  pages={485--508},
  year={2012},
  publisher={Annual Reviews}
}
@inproceedings{compressed_sensing_rnn,
    title={A Compressed Sensing View of Unsupervised Text Embeddings, Bag-of-n-Grams, and {LSTM}s},
    author={Sanjeev Arora and Mikhail Khodak and Nikunj Saunshi and Kiran Vodrahalli},
    booktitle={International Conference on Learning Representations},
    year={2018},
    url={https://openreview.net/forum?id=B1e5ef-C-},
}

@article{zhang_empirical_nodate,
	title = {An {Empirical} {Study} of {Learning} from {Imbalanced} {Data}},
	abstract = {No consistent conclusions have been drawn from existing studies regarding the eﬀectiveness of diﬀerent approaches to learning from imbalanced data. In this paper we apply bias-variance analysis to study the utility of diﬀerent strategies for imbalanced learning. We conduct experiments on 15 real-world imbalanced datasets of applying various re-sampling and induction bias adjustment strategies to the standard decision tree, naive bayes and k-nearest neighbour (k-NN) learning algorithms. Our main ﬁndings include: Imbalanced class distribution is primarily a high bias problem, which partly explains why it impedes the performance of many standard learning algorithms. Compared to the re-sampling strategies, adjusting induction bias can more signiﬁcantly vary the bias and variance components of classiﬁcation errors. Especially the inverse distance weighting strategy can signiﬁcantly reduce the variance errors for k-NN. Based on these ﬁndings we oﬀer practical advice on applying the re-sampling and induction bias adjustment strategies to improve imbalanced learning.},
	author = {Zhang, Xiuzhen and Li, Yuxuan},
	pages = {9}
}
@article{chung_empirical_2014,
	title = {Empirical {Evaluation} of {Gated} {Recurrent} {Neural} {Networks} on {Sequence} {Modeling}},
	url = {http://arxiv.org/abs/1412.3555},
	abstract = {In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.},
	urldate = {2018-01-26TZ},
	journal = {arXiv:1412.3555 [cs]},
	author = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.3555},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing}
}
@misc{noauthor_sustainable_nodate,
	title = {Sustainable development goals - {United} {Nations}},
	url = {http://www.un.org/sustainabledevelopment/sustainable-development-goals/},
	abstract = {Full list of the 17 proposed sustainable development goals and summaries of their targets},
	language = {en-US},
	urldate = {2018-01-25TZ},
	journal = {United Nations Sustainable Development}
}
