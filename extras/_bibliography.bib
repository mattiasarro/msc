@misc{murphy,
  title={Machine learning: a probabilistic perspective},
  author={Kevin, Murphy},
  year={2012},
  publisher={The MIT press}
}
@article{adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}
@book{dlb,
  title={Deep learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
  volume={1},
  year={2016},
  publisher={MIT press Cambridge}
}
@misc{colah,
  author = {Christopher Olah},
  title = {{Understanding LSTM Networks}},
  howpublished = "\url{http://colah.github.io/posts/2015-08-Understanding-LSTMs/}",
  year = {2015},
  note = "[Online; accessed 30-March-2018]"
}
@misc{vanishing,
  author = {Harini Suresh},
  title = {{Vanishing Gradients \& LSTMs}},
  howpublished = "\url{http://harinisuresh.com/2016/10/09/lstms/}",
  year = {2016},
  note = "[Online; accessed 30-March-2018]"
}
@article{lstm,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}
@article{gru,
  title={Learning phrase representations using RNN encoder-decoder for statistical machine translation},
  author={Cho, Kyunghyun and Van Merri{\"e}nboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1406.1078},
  year={2014}
}
@article{1dcnn,
  Author = {Kim, Yoon},
  Journal = {arXiv preprint arXiv:1408.5882},
  Title = {Convolutional Neural Networks for Sentence Classification},
  Year = {2014}
}
@article{dilated,
  author    = {Fisher Yu and
               Vladlen Koltun},
  title     = {Multi-Scale Context Aggregation by Dilated Convolutions},
  journal   = {CoRR},
  volume    = {abs/1511.07122},
  year      = {2015},
  url       = {http://arxiv.org/abs/1511.07122},
  archivePrefix = {arXiv},
  eprint    = {1511.07122},
  timestamp = {Wed, 07 Jun 2017 14:40:43 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/YuK15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{dilated_decoder,
  title={Improved variational autoencoders for text modeling using dilated convolutions},
  author={Yang, Zichao and Hu, Zhiting and Salakhutdinov, Ruslan and Berg-Kirkpatrick, Taylor},
  journal={arXiv preprint arXiv:1702.08139},
  year={2017}
}

@article{googlenet,
  author    = {Christian Szegedy and
               Wei Liu and
               Yangqing Jia and
               Pierre Sermanet and
               Scott E. Reed and
               Dragomir Anguelov and
               Dumitru Erhan and
               Vincent Vanhoucke and
               Andrew Rabinovich},
  title     = {Going Deeper with Convolutions},
  journal   = {CoRR},
  volume    = {abs/1409.4842},
  year      = {2014},
  url       = {http://arxiv.org/abs/1409.4842},
  archivePrefix = {arXiv},
  eprint    = {1409.4842},
  timestamp = {Wed, 07 Jun 2017 14:40:42 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/SzegedyLJSRAEVR14},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{bagging,
  title={Bagging predictors},
  author={Breiman, Leo},
  journal={Machine learning},
  volume={24},
  number={2},
  pages={123--140},
  year={1996},
  publisher={Springer}
}
@article{adaboost,
  title={A decision-theoretic generalization of on-line learning and an application to boosting},
  author={Freund, Yoav and Schapire, Robert E},
  journal={Journal of computer and system sciences},
  volume={55},
  number={1},
  pages={119--139},
  year={1997},
  publisher={Elsevier}
}
@article{dropout,
  title={Dropout: A simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The Journal of Machine Learning Research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={JMLR. org}
}
@article{wide_deep,
  author    = {Heng{-}Tze Cheng and
               Levent Koc and
               Jeremiah Harmsen and
               Tal Shaked and
               Tushar Chandra and
               Hrishi Aradhye and
               Glen Anderson and
               Greg Corrado and
               Wei Chai and
               Mustafa Ispir and
               Rohan Anil and
               Zakaria Haque and
               Lichan Hong and
               Vihan Jain and
               Xiaobing Liu and
               Hemal Shah},
  title     = {Wide {\&} Deep Learning for Recommender Systems},
  journal   = {CoRR},
  volume    = {abs/1606.07792},
  year      = {2016},
  url       = {http://arxiv.org/abs/1606.07792},
  archivePrefix = {arXiv},
  eprint    = {1606.07792},
  timestamp = {Tue, 31 Oct 2017 11:52:28 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/ChengKHSCAACCIA16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{mvdl,
 author = {Elkahky, Ali Mamdouh and Song, Yang and He, Xiaodong},
 title = {A Multi-View Deep Learning Approach for Cross Domain User Modeling in Recommendation Systems},
 booktitle = {Proceedings of the 24th International Conference on World Wide Web},
 series = {WWW '15},
 year = {2015},
 isbn = {978-1-4503-3469-3},
 location = {Florence, Italy},
 pages = {278--288},
 numpages = {11},
 url = {https://doi.org/10.1145/2736277.2741667},
 doi = {10.1145/2736277.2741667},
 acmid = {2741667},
 publisher = {International World Wide Web Conferences Steering Committee},
 address = {Republic and Canton of Geneva, Switzerland},
 keywords = {deep learning, multi-view learning, recommendation system, user modeling},
}
@inproceedings{cdl,
  title={Collaborative deep learning for recommender systems},
  author={Wang, Hao and Wang, Naiyan and Yeung, Dit-Yan},
  booktitle={Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  pages={1235--1244},
  year={2015},
  organization={ACM}
}
@inproceedings{dl_mf,
  title={VBPR: Visual Bayesian Personalized Ranking from Implicit Feedback.},
  author={He, Ruining and McAuley, Julian},
  booktitle={AAAI},
  pages={144--150},
  year={2016}
}
@article{vae,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max},
  journal={arXiv preprint arXiv:1312.6114},
  year={2013}
}
@inproceedings{semi_vae,
  title={Semi-supervised learning with deep generative models},
  author={Kingma, Diederik P and Mohamed, Shakir and Rezende, Danilo Jimenez and Welling, Max},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3581--3589},
  year={2014}
}
@article{gumbel,
  title={Categorical reparameterization with gumbel-softmax},
  author={Jang, Eric and Gu, Shixiang and Poole, Ben},
  journal={arXiv preprint arXiv:1611.01144},
  year={2016}
}
@misc{vae_bad,
  author = {Brian Keng},
  title = {{Semi-supervised Learning with Variational Autoencoders}},
  howpublished = "\url{http://bjlkeng.github.io/posts/semi-supervised-learning-with-variational-autoencoders/}",
  year = {2017},
  note = "[Online; accessed 1-April-2018]"
}
@InProceedings{towards,
  title = 	 {Toward Controlled Generation of Text},
  author = 	 {Zhiting Hu and Zichao Yang and Xiaodan Liang and Ruslan Salakhutdinov and Eric P. Xing},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1587--1596},
  year = 	 {2017},
  editor = 	 {Doina Precup and Yee Whye Teh},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {International Convention Centre, Sydney, Australia},
  month = 	 {06--11 Aug},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/hu17e/hu17e.pdf},
  url = 	 {http://proceedings.mlr.press/v70/hu17e.html},
  abstract = 	 {Generic generation and manipulation of text is challenging and has limited success compared to recent deep generative modeling in visual domain. This paper aims at generating plausible text sentences, whose attributes are controlled by learning disentangled latent representations with designated semantics. We propose a new neural generative model which combines variational auto-encoders (VAEs) and holistic attribute discriminators for effective imposition of semantic structures. The model can alternatively be seen as enhancing VAEs with the wake-sleep algorithm for leveraging fake samples as extra training data. With differentiable approximation to discrete text samples, explicit constraints on independent attribute controls, and efficient collaborative learning of generator and discriminators, our model learns interpretable representations from even only word annotations, and produces short sentences with desired attributes of sentiment and tenses. Quantitative experiments using trained classifiers as evaluators validate the accuracy of sentence and attribute generation.}
}

@inproceedings{compressive_sensing1,
  author    = {Richard G. Baraniuk},
  title     = {Compressive sensing},
  booktitle = {42nd Annual Conference on Information Sciences and Systems, {CISS}
               2008, Princeton, NJ, USA, 19-21 March 2008},
  year      = {2008},
  url       = {https://doi.org/10.1109/CISS.2008.4558479},
  doi       = {10.1109/CISS.2008.4558479},
  timestamp = {Fri, 19 May 2017 01:25:12 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/ciss/Baraniuk08},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{compressive_sensing2,
  abstract = {This paper considers the model problem of reconstructing an object from incomplete frequency samples. Consider a discrete-time signal f&isin;C<sup>N</sup> and a randomly chosen set of frequencies &Omega;. Is it possible to reconstruct f from the partial knowledge of its Fourier coefficients on the set &Omega;? A typical result of this paper is as follows. Suppose that f is a superposition of |T| spikes f(t)=&sigma;<sub>&tau;&isin;T</sub>f(&tau;)&delta;(t-&tau;) obeying |T|&le;C<sub>M</sub>&middot;(log N)<sup>-1</sup> &middot; |&Omega;| for some constant C<sub>M</sub>>0. We do not know the locations of the spikes nor their amplitudes. Then with probability at least 1-O(N<sup>-M</sup>), f can be reconstructed exactly as the solution to the &#8467;<sub>1</sub> minimization problem. In short, exact recovery may be obtained by solving a convex optimization problem. We give numerical values for C<sub>M</sub> which depend on the desired probability of success. Our result may be interpreted as a novel kind of nonlinear sampling theorem. In effect, it says that any signal made out of |T| spikes may be recovered by convex programming from almost every set of frequencies of size O(|T|&middot;logN). Moreover, this is nearly optimal in the sense that any method succeeding with probability 1-O(N<sup>-M</sup>) would in general require a number of frequency samples at least proportional to |T|&middot;logN. The methodology extends to a variety of other situations and higher dimensions. For example, we show how one can reconstruct a piecewise constant (one- or two-dimensional) object from incomplete frequency samples - provided that the number of jumps (discontinuities) obeys the condition above - by minimizing other convex functionals such as the total variation of f.},
  added-at = {2013-05-30T04:20:11.000+0200},
  author = {Cand{\`e}s, Emmanuel J. and Romberg, Justin and Tao, Terence},
  biburl = {https://www.bibsonomy.org/bibtex/2f3e63909ebf5bdfdb7e841179e704c93/ytyoun},
  doi = {10.1109/TIT.2005.862083},
  interhash = {ff0f531efa2260488be3523d4e72a99d},
  intrahash = {f3e63909ebf5bdfdb7e841179e704c93},
  issn = {0018-9448},
  journal = {Information Theory, IEEE Transactions on},
  keywords = {compressed.sensing uncertainty},
  number = 2,
  pages = {489--509},
  timestamp = {2015-12-10T09:45:43.000+0100},
  title = {Robust Uncertainty Principles: Exact Signal Reconstruction from Highly Incomplete Frequency Information},
  volume = 52,
  year = 2006
}
@article{compressive_sensing3,
  title={Compressed sensing, sparsity, and dimensionality in neuronal information processing and data analysis},
  author={Ganguli, Surya and Sompolinsky, Haim},
  journal={Annual review of neuroscience},
  volume={35},
  pages={485--508},
  year={2012},
  publisher={Annual Reviews}
}
@inproceedings{compressed_sensing_rnn,
    title={A Compressed Sensing View of Unsupervised Text Embeddings, Bag-of-n-Grams, and {LSTM}s},
    author={Sanjeev Arora and Mikhail Khodak and Nikunj Saunshi and Kiran Vodrahalli},
    booktitle={International Conference on Learning Representations},
    year={2018},
    url={https://openreview.net/forum?id=B1e5ef-C-},
}

@article{zhang_empirical_nodate,
	title = {An {Empirical} {Study} of {Learning} from {Imbalanced} {Data}},
	abstract = {No consistent conclusions have been drawn from existing studies regarding the eﬀectiveness of diﬀerent approaches to learning from imbalanced data. In this paper we apply bias-variance analysis to study the utility of diﬀerent strategies for imbalanced learning. We conduct experiments on 15 real-world imbalanced datasets of applying various re-sampling and induction bias adjustment strategies to the standard decision tree, naive bayes and k-nearest neighbour (k-NN) learning algorithms. Our main ﬁndings include: Imbalanced class distribution is primarily a high bias problem, which partly explains why it impedes the performance of many standard learning algorithms. Compared to the re-sampling strategies, adjusting induction bias can more signiﬁcantly vary the bias and variance components of classiﬁcation errors. Especially the inverse distance weighting strategy can signiﬁcantly reduce the variance errors for k-NN. Based on these ﬁndings we oﬀer practical advice on applying the re-sampling and induction bias adjustment strategies to improve imbalanced learning.},
	author = {Zhang, Xiuzhen and Li, Yuxuan},
	pages = {9}
}
@article{chung_empirical_2014,
	title = {Empirical {Evaluation} of {Gated} {Recurrent} {Neural} {Networks} on {Sequence} {Modeling}},
	url = {http://arxiv.org/abs/1412.3555},
	abstract = {In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.},
	urldate = {2018-01-26TZ},
	journal = {arXiv:1412.3555 [cs]},
	author = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.3555},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing}
}
@misc{noauthor_sustainable_nodate,
	title = {Sustainable development goals - {United} {Nations}},
	url = {http://www.un.org/sustainabledevelopment/sustainable-development-goals/},
	abstract = {Full list of the 17 proposed sustainable development goals and summaries of their targets},
	language = {en-US},
	urldate = {2018-01-25TZ},
	journal = {United Nations Sustainable Development}
}
