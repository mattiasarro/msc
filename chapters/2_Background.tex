\chapter{Background}


\section{Input Representations}
\label{data_rep}

\subsection{Categorical Input}

The simplest option for representing categorical input is 1-hot encoding, where input has as many dimensions as there are distinct categorical values (vocabulary size); a single dimension is set to 1, with all other dimensions set to 0.
This is a straightforward representation: for a simple model such as logistic regression, we can clearly interpret the model parameters and see how the presence or absence of a given category increases or decreases the likelihood of a given output.
However, in cases where vocabulary sizes get  large, and the number of outputs (the number of units in the next layer in a deep network,  or the number of output units  in a shallow one) increases, this kind of encoding can really blow up the number of parameters of the model.

An alternative is to use  random embeddings:  dense, low-dimensional representations of a high-dimensional vectors.
It has been shown in compressed sensing literature that if a high-dimensional signal in effect lies on a low-dimensional manifold, then the original signal can be reconstructed from a small number of linear measurements \cite{compressive_sensing2}.
This has a useful implication for machine learning: categorical variables with large vocabularies of size \textit{d}  can be  represented with  random embedding vectors of size $M = \log(d)$.
This is because it is possible to reconstruct any d-dimensional k-sparse signal using at most $k  log{\frac{d}{k}}$ dimensional vectors \cite{compressive_sensing1}, and 1-hot vectors are 1-sparse (only one dimension is nonzero).

An intuitive example is given in \cite{compressive_sensing3} about natural images: a 1-million-pixel image could have roughly 20,000 edges\footnote{technically: wavelet coefficients with a significant power, which roughly corresponds to a superimposition of edges}, i.e. it lies in a roughly 20,000-dimensional manifold from which the original image can be constructed with high fidelity.
The 1-million-dimensional image could be projected into a M-dimensional space ``by taking M measurements of the image, where each measurement consists of a weighted sum of all the pixel intensities, and allowing the weights themselves to be chosen randomly (for example, drawn independently from a Gaussian distribution)''\cite{compressive_sensing3}.
The projection is random because the weights for the sum of pixel intensities are chosen randomly.
Why the projections need to be random is motivated by another intuitive example:  when light is shined on a 3-dimensional wireframe, its shadow is a 2-dimensional projection of it.
The projection could lose some important information about the regional object if the direction of the light source is chosen poorly,  however random directions are likely to result in projections where every link of the wire will have a corresponding nonzero length of shadow.

\subsection{Text Input}

The simplest way to represent  text is bag words (\textbf{BoW}),  which  is simply the histogram of word occurrences  in the text.
BoW  representations give equal weight to each of the words in the text, and the model has to learn the relative importance of each of these.
A commonly used representation in information retrieval (IR) is \textbf{TF-IDF},  which stands for ``term frequency -  inverse document frequency''.
TF-IDF  multiplies the term frequency (number of times the word occurred in the text) with its inverse document frequency (the inverse of how often a occurs in all documents).
This has the effect of giving lower scores for common words, and higher scores for words which appear often in a document but not too frequently in the whole corpus.
Another common representation is bag of n-grams (\textbf{BonG}), which  is a histogram of character n-grams.

The above representations are sparse, like 1-hot encoding of categorical variables.
Text can also be  represented as a sequence of \textbf{word embeddings}.
Word embeddings can either be random vectors or representations  learned with word cooccurrence algorithms such as word2vec \cite{word2vec} and GloVe \cite{glove};  these representations can remain fixed throughout the learning procedure, or updated as part of the stochastic gradient descent (SGD) when applicable.
A simple way to represent text is to average the word embedding contained in it.
The  average could also be weighted by the TF-IDF score of each word\footnote{the author is unaware whether this has been tried before, but it seems a promising approach}.
In \cite{doc2vec}, paragraph vectors are created in a similar manner to word2vec vectors by relying on these vectors to predict the next word in a sentence.

More complex methods use recurrent neural networks (\textbf{RNNs}) to combine a sequence of word embeddings  into a fixed length vector (see section \ref{rnn}).
These kinds of models are good at  disambiguating the potentially many meanings a word might have.
Exactly what gets persisted in the final  vector representation of text depends on how the model is trained - two RNNs  trained on different tasks (such as sentiment analysis and named entity recognition)  would need to store different information in its hidden state to successfully  perform their relevant tasks,  hence the encoding for the same sentence would be different for either model.
Still, an RNN  that is trained on one task could provide useful features for another.
Encoding text as fixed length vectors using RNNs has been interpreted as compressed sensing \cite{compressed_sensing_rnn}, with such vectors being ``provably at least as powerful on classification tasks, up to small error, as a linear classifier over BonG vectors''.

\subsection{Image Input}

Images can be represented as dense 3-dimensional tensor.
A 28x28 RGB image could be represented as a tensor with shape [28, 28, 3],  with the final dimension corresponding to the green, red, and blue intensity values at a given x/y coordinate.
These intensities  are usually in the range 0 ... 255, but are normalised before input to machine learning model.
Similarly to RNNs  encoding a sequence of words to a vector, 2D convolutional neural networks (2D CNNs)  can be used to extract dense feature vectors from raw images (image embedding),  further discussed in section \ref{image_models}.

\section{Models Considered}
\label{models_considered}

The following sections describes a selection of models that could be used for our classification task.
This is by no means an exhaustive list, nor is it a selection that is expected to have the highest predictive performance individually.
These models might become part of an ensemble, where diversity matters much more than the performance of any individual model.
Engineering  considerations and time constraints also influence the selection of models: we did not want to spend time implementing nontrivial models from scratch, or to use many frameworks for training models.
TensorFlow  is a widely used machine learning library with a good selection of open source models, in particular a selection of pre-trained models for computer vision and NLP, and has arguably the most mature deployment ecosystem.
Therefore models that did not have an open source TensorFlow implementation were automatically excluded.

Section \ref{general_models} describes models that can take categorical and text inputs, section \ref{text_models} looks at some neural models that take only text inputs, and section \ref{image_models} describes 2D CNNs that can classify or extract features from images.

\subsection{General Models}
\label{general_models}



\subsubsection{Logistic Regression}

Logistic regression is a linear, discriminative classifier that is a common baseline model, since it is both interpretable and efficient to train.
There are methods that take time linear in the number of non-zeros in the dataset, which is the smallest amount possible, and it can be made to handle non-linear decision boundaries by using kernels \cite{murphy}.

We have two cases: for independent categorical outputs we use a separate binomial logistic regression model for each output class, and for mutually exclusive classes we use multinomial logistic regression.
This is shown formally in eq  \ref{logistic} and \ref{logistic_binom}, where where \textit{X} and \textit{W} correspond to the input and weight matrices, respectively.

We follow the notational trick where the first row of \textit{X} is always 1, and the first row of \textit{W} corresponds to the bias term.
The loss function of this model as well as how it is optimised is described in section \ref{loss}.

\begin{equation}
\label{logistic}
p(y|X,W)=\mathrm{sigmoid}(W^TX) \
\end{equation}

% todo
\begin{equation}
\label{logistic_binom}
p(y|X,W)=\mathrm{softmax}(W^TX) \
\end{equation}

\subsubsection{Feedforward Neural Network}

Feedforward neural networks (also called deep feedforward networks, multi-layer perceptrons) have been described as function approximator, whose goal is to approximate some functions $f^*$.
We expect some familiarity with neural networks from the reader; for an excellent overview of the various use cases and models of deep learning, refer to \cite{dlb}.
In our case, the input is some representation of categorical, text or image input - often an embedding extracted with another model or assigned randomly.
The hidden layers of our network are homogenous: they use the same activation function (ReLu, sigmoid, or tanh) and have the same number of units;  activation functions and the number of units in hidden layers are determined through hyperparameter tuning.
Similarly to logistic regression, we have two cases: sigmoid activation for independent categories and softmax layer for mutually exclusive categories.

\subsubsection{Wide \& Deep}

Wide and deep model was originally proposed for recommender systems \cite{wide_deep}, but can just as well be used for classification.
In this model a deep network and a linear logistic regression model are trained jointly in the same SGD learning process, rather than trained separately and then ensembled.
The wide component is good at remembering ``feature interactions through a wide set of cross-product feature transformation'' while the deep model provides good generalisation.
In our experiments we did not use feature cross products of features.

\subsubsection{Loss Function and Optimizers}
\label{loss}

Logistic regression and neural networks are optimised with some variant of gradient descent.
The loss function was the same for multi-label and multi-class training objectives: binary cross-entropy between the training data and the model distribution, with the loss value averaged across all classes.
This is given in eq. \ref{xentropy}, where $\theta$ corresponds to the model parameters such as the weights and biases of the model, $C$ corresponds to the number of classes, and $P(c=1|x, \theta)$ gives the predicted probability that the item belongs to class $c$ given the feature vector $x$.

\begin{align}
  \label{xentropy}
  NLL(\theta) &= \sum\limits_{c=1}^C \sum\limits_{i=1}^N\left[c_i\log P(c=1|x, \theta) + (1-c_i)\log(P(c=0|x, \theta) )\right]
\end{align}

We are minimising the negative log likelihood (NLL) rather than maximising the product of likelihoods.
Multiplying large numbers of probabilities could result in numerical underflow and rounding errors, therefore it is pragmatic to work with sums of log probabilities instead.
The Adam \cite{adam} optimiser works very well with default hyperparameters, and is therefore used for all stochastic gradient descent updates.

Cross-entropy is a standard loss function for multi-class and multi-label classification, however in our cases it may have downsides.
There is at times ambiguity as to which category a product should belong, therefore even hand-assigned labels are somewhat arbitrary.
The similarity between categories is ignored by this loss function: a short coat that is misclassified as an earring has the same loss value as a coat that is classified as a jacket.
Additionally, the rule-based labels are incomplete: the lack of a label does not always mean that the product does not belong to the given category, yet the model gets penalised for cases where such label is missing but the model correctly predicted a semantically similar class.

The Wasserstein distance metric (also called earth mover distance) can be used instead, which gives the cost of the optimal transport plan for moving the mass of one probability distribution to another.
In our case the two distributions are the label distribution of a data point (1-hot / k-hot vector) and the class distribution predicted by the model.
The optimal transport plan is weighted by a $KxK$ distance matrix, where $K$ is the number of classes and the entries in the matrix correspond to how dissimilar the classes are\footnote{Similarity is known \textit{a priori}, e.g. derived from WordNet hierarchy. It would be interesting to explore similarity matrices created using cosine distance of embedding vectors of categories; embeddings could be extracted with an RNN from the description of the category}.
Intuitively, transporting probability mass from the category ``Coats'' to ``Earrings'' should be more expensive than from ``Coats'' to ``Jackets''.

% TODO describe why we won't use this loss

\subsection{Text Models}
\label{text_models}

\subsubsection{1D Convolutional Neural Networks}

A simple convolutional architecture for text classification is described in \cite{1dcnn}.
Pre-trained $k$-dimensional word vectors are concatenated to represent a sentence, and a filter is $w \in R^{hk}$ is applied to a window of $h$ words at each possible window of words in a sentence; this gives a single variable-length feature vector representing the sentence.
Several such filters are learned (each with potentially a different width $h$), and max-over-time pooling is applied to these feature maps; this gives a vector of the highest activations from each filter, which is then passed to a fully-connected softmax layer for final classification (in the case of multi-class classification).
For multi-label classification, the final layer would be a fully-connected layer of sigmoid activations.

This simple architecture should be able to predict output classes reasonably.
Each filter can indicate the presence of a a sequence of $h$ words; max-pooling discards information about where exactly in the text it appeared, and ensures the output is of a fixed length.
The same filter $w$ is used across all possible word windows in the sentence, which can be seen as a form of parameter sharing (and as an infinitely strong prior over the parameters of the model \cite{dlb}), and enables processing variable-length sequences.
The relatively small number of shared parameters requires less training data than an equivalent fully-connected architecture.
Using pre-trained word embeddings further simplifies the task, as these already carry some information about the meaning or syntactic role of words.

More sophisticated architectures can be built using 1D CNNs, although not used in our experiments.
Most notably, several layers of convolutions (each optionally followed by pooling) can be stacked, where the following layer works on the feature maps output by the previous layer.
The model described above uses a stride and dilation of 1, but multi-layer architectures where the higher layers have use exponentially increasing dilation are able to expand the receptive field of the model, and as a result aggregate ``contextual information from multiple scales'' \cite{dilated}.
Dilated convolutions were beneficial for semantic segmentation of images with 2D CNNs, but this increased receptive field has been useful for sequence-to-sequence models in NLP \cite{dilated_decoder}.

\subsubsection{Recurrent Neural Networks}
\label{rnn}

Recurrent neural networks (RNNs) are a family of models for processing sequential data.
The sequence of inputs $x^{(1)} \dots x^{(t)}$ in our case are vectors of word embeddings, but there are other options for input representations (e.g. sequence of 1-hot encoded tokens, or vectors representing a multivariate time series at a given time step $t$).
Vanilla RNNs maintain a hidden state vector $h$ which contains some information about the sequence it has seen so far, and is calculated from the input at the current time step $t$, and the previous hidden state:

\begin{equation}
  h^{(t)} = f(h^{(t-1)},x^{(t)})
\end{equation}

Vanilla RNNs suffer from the vanishing and exploding gradient problem in long sequences and are notoriously difficult to train.
Probably the most widely used variant, the long short-term memory networks (LSTM) \cite{lstm} overcome this limitation by augmenting the network with a memory cell $c$; there is a learned gating mechanism that controls what part of (and the extent to which) the cell is forgotten, what gets persisted in the cell state, and what gets output at a given time step.
An intuitive overview of the gating mechanisms is given by \cite{colah} and \cite{vanishing} describes well how this helps mitigate vanishing gradients.
A simpler gating mechanism is offered by gated recurrent units (GRU) \cite{gru}, which has fewer parameters and works well on simpler tasks.

We are interested in using RNNs as text encoders, though they are often used for sequence to sequence modelling, or for predicting an output at each time step.
In the simplest case, the concatenation of the cell state $c$ and the hidden state $h$ at the last time step could be used as a representation of the sequence.

\subsection{Image Models}
\label{image_models}

2D CNNs are important historically - excellent results in computer vision revived interest and funding in deep learning - but also applicable to a wide range of problems.
Computer vision CNNs tend to be complex models with millions of parameters and dozens (or even hundreds) of layers.
It is very compute-intensive and time-consuming to search for new architectures, and the analysis of these is beyond the scope of this work.
Our goal is to use a CNN that has a reasonable performance without incurring unreasonable overhead.

The general architecture of a 2D CNN is as follows.
Patches of feature extractors are convolved over the x and y axis of the image; these are often small, e.g. 3x3x$c$ patches that span all the $c$ channels of the image ($c=3$ for RGB images), and convolution could be strided.
Former models used larger patch sizes, but stacking several smaller patches could achieve similar receptive fields while allowing for a larger number of non-linearities for the same number of parameters.
One or more layers of convolutions would be followed by a (often max) pooling layer.
Computer vision architectures tend to re-use the same sub-structure repeatedly: either the same kinds of convolutions followed by pooling, or the ``network in a network'' structure popularised by GoogLeNet \cite{googlenet}.
After a number of such repeated structures, there are normally a few fully-connected layers followed by a softmax or sigmoid layer.

Transfer learning is particularly applicable to computer vision.
Successful computer vision models require a notoriously large training datasets, which would be hard to come by for every problem.
Instead, models are often initialised to the values of a model that has been trained on a large dataset such as ImageNet \cite{imagenet}, excluding only the last layer(s) of the model that has learned features that are specific to the original problem.
Representations learned at the lower layers are remarkably  universal and useful for other tasks.
Therefore pre-trained 2D CNNs can be used as feature extractors for various other tasks,  or the models could be fine-tuned to learn representations that are even more useful to the new task.

\subsection{Downstream Models}

\subsubsection{Item Similarity}
\label{bg_sim}

Similarity measures based on tokenised text are easily accessible to e-commerce companies: the popular open source NoSQL database ElasticSearch (which makes use of Lucene, a full-text search engine) provides document similarity metrics that are some combination of normalised TF-IDF vectors between all documents and a query (which could be another document).
This gives decent results in many cases, but it does not take advantage of a product image.

Embeddings extracted with a deep network can be viewed as a distributed representation of the original data, which carries semantic meaning \cite{distributed_reps}.
In the contrived examples of distributed representations, each dimension would represent the degree to which a property (e.g. of shape or colour) is present of an object, however the dimensions are not necessarily so nicely disentangled in the representations learned by deep networks\footnote{The author has not seen strong evidence in literature to support either case, but at least in \cite{towards} extra steps had to be taken to \textit{prevent} certain latent variables from encoding properties of the data that were designed to be captured by other latent variables. This implies that by default, each latent variable contains a little bit of information about each aspect of the input, and that the whole pattern of the embedding matters for conveying semantic information}.
Still, it is clear that such dense representations help linear models do accurate predictions, and semantically similar data points will have similar embedding vectors.

Therefore we can compare any two embedding vectors (extracted with the same neural network) using standard vector space similarity measures, such as cosine similarity (1 - normalised dot product between the vectors).
Such embeddings can be extracted from product images with 2D CNNs pre-trained on another task, 1D CNNs or RNNs on product title and descriptions, or from the joint embeddings described in section \ref{bg_ensembling}.
With large numbers of products, calculating pairwise similarity scores becomes intractable, therefore approximate nearest neighbour methods could be used \cite{nmslib}.

Another interesting method of computing similarity scores between documents utilises the full-text search capability of tools such as Lucene, which would be valuable to firms already using such technologies.
The embedding vectors of products could be tokenised, by converting each dimension of each embedding vector into tokens that represent the feature with some precision \cite{vec_fulltext}.
For example, we could consider two levels of precision and divide the normalised feature into 10 intervals of size 0.1 and 100 intervals of size 0.01; each product would get two tokens per embedding dimension\footnote{It would be a good idea to also have some wider overlapping intervals as well, e.g. 0 ... 0.5}.
For example, if the first embedding dimension is 0.46, the inserted tokens would be d1\_0.4 and d1\_0.46.
Full-text search engines would assign higher scores to co-occurrences in the more precise tokens and lower scores to co-occurrences in more coarse token.

\subsubsection{Recommender Systems}
\label{rec}

Several methods have been proposed that use dense embeddings learned by deep models as part of a recommender algorithm.
These are briefly described here to motivate our focus on models that obtain dense embeddings of products.

Models that deal exclusively with deep embeddings \cite{mvdl} require huge amounts of user-product feedback pairs, therefore the more interesting solutions are ones which work together with classical methods such as matrix factorisation (MF).
MF finds - purely from the implicit or explicit feedback users give to items - vectors of latent factors that represent each user's preferences and each item's ``qualities''; the inner product between a user's and item's latent factors gives a score of compatibility between the two.
Deep models can augment the item latent factors by injecting the embeddings learned for another task
The main difference in these methods is how the deep embeddings are merged into MF, and how the deep embeddings are obtained in the first place.
In \cite{cdl}, stacked denoising autoencoders are used for unsupervised feature learning of item data, while \cite{dl_mf} uses simply visual embeddings extracted from a 2D CNN pre-trained on ImageNet.

\section{Unsupervised and Semisupervised Learning}
\label{unsup}

This section explores ways in which unsupervised and semi-supervised learning can learn good feature representations and improve sample complexity.
Generic semi-supervised methods such as self-training are not considered, as one of our goals is to learn good representations, but also because these methods can reinforce poor predictions or do not make full use of all available unlabelled data.

Some generic methods can still improve the representations learned by our models when applied to models that learn deep embeddings in order to make predictions.
Entropy minimisation \cite{entropy_min} can be incorporated into models trained with SGD to encourage confident predictions of each class; this encourages the deep model to learn predictions that are strong predictors of some class and avoid producing features that produce mixed predictions.
More complex but very performant approach is Mean Teacher \cite{mean_teacher}. A student that gets a harder task (such as predicting from a noisy/adversarial example), and a teacher that gets an easier task (i.e. the teacher model is an ensemble, which is more accurate).
The student is trained on the prediction of the teacher; the teacher's parameters are an exponential moving average of the student's, updated after each minibatch.
The teacher's predictions are higher-quality than the student's (and can be applied on unlabelled data), while the student tries to continuously learn to learn a predictor that is robust to noisy and adversarial examples.

\subsection{Autoencoders \& Variational Autoencoders}

Deep autoencoders (AEs) can be used to learn low-dimensional representations of inputs.
Many variants exist,  but  the general pattern is to have an ``hourglass-structured'' neural network where the first half of the model shrinks the input, and the second half of the network reconstructs the original input.
Activations in the middle layer correspond to a dense embedding of the sample;  the shrinking layers need to  throw away some of the detail in the input, yet persists enough for the expanding layers to be able to reconstruct the original input with some fidelity.
The embedding contains information that is mostly unique to the data point, while the parameters of the encoding and decoding layers ensure that the reconstructed input is realistic.
It is common to add noise (Gaussian, dropout) to the input or the intermediate layers to increase resilience to noisy inputs and to prevent the autoencoder simply memorising each data point.

If the bottleneck layer is unconstrained, it will use a wide range of values to represent different inputs.
We would like embedding for similar inputs to also be similar, which is not always the case for ordinary autoencoders.
Variational autoencoders (VAEs) impose a prior distribution on the values of the embedding, often a multivariate Gaussian distribution with a diagonal covariance matrix, and the model is regularised during training to respect this prior.
The model is optimised to minimise the reconstruction loss and KL divergence between the model's distribution $z$ and the prior on it, which can be computed just from $\mu$ and $\sigma$ if the prior is a isotropic standard normal.
Enforcing the prior also means that the embeddings $z$ will occupy a smooth, contiguous space, which allows us to draw samples from the prior $\epsilon \sim \mathcal{N}(0, 1)\,$ and use that as the input to the decoder - this gives us a generative model, which would not be possible with ordinary autoencoders.
The output of the VAE is also probabilistic: e.g. for images, the output for a pixel would be a Gaussian distribution, which is sampled the same way as the Gaussians for $z$.

The VAEs described here are probabilistic models parameterised by neural networks for approximating the true posterior $p(z|x)$, since the exact posterior is intractable.
In practice, VAEs add an extra loss term (KL divergence) to AEs, and additional step to determining the embedding $z$.
There are two separate neural network layers from the bottleneck layer of the AE:  one for determining $\mu$ (the vector of means of the multivariate Gaussian), and one for determining $\sigma$ (the vector of standard deviations of the multivariate gaussian).
Given $\mu$ and $\sigma$, we can sample the final item embedding $z \sim \mathcal{N}(\mu,\,\sigma^{2})\,$.
Note that sampling of $z$ is a discrete decision that would normally stop gradient from propagating past this step, but we can use the reparametrisation trick introduced by \cite{vae} that turns the discrete decision into a deterministic function of $z = \mu + \sigma\epsilon$, where $\epsilon \sim \mathcal{N}(0, 1)\,$ is a random auxiliary noise parameter.


\subsection{Conditional Variational Autoencoders}
\label{gans}
It is easier for linear models to learn from embeddings extracted with AEs, and embeddings from VAEs make the classes even easier to separate.
These are unsupervised methods that do not take advantage of labels in the training data.
In the semisupervised approach introduced by \cite{semi_vae} there are two inference networks: a discriminative classifier that outputs a categorical distribution from the input $x$, and a class-conditional encoder that takes as input both $x$ and the 1-hot encoded categorical label $y$\footnote{class-conditional encoder means that the encoder is aware of the class of the data point, i.e. the output distribution $z$ is conditioned on the class $y$ in addition to the original input $x$}.
The model is trained on both labelled and unlabelled data.
For labelled examples, the $x$ and $y$ are given as input to the model, which embeds it in $z$ as described earlier, and reconstructs the original $x$.
The value of $y$ is unknown for unlabelled data points; therefore the model is run $|y|$ times, encoding and decoding the data point conditioned on each possible class $i$; loss from these runs is averaged, weighted by the discriminator's estimate of the sample belonging to class $i$.
This approach is acceptable for small numbers of classes, but is impractical for many multi-class problems.
A solution uses the reparametrisation trick mentioned above: rather than running the procedure once per class, we take a Gumbel-softmax \cite{gumbel} to get a discrete estimate of $y$ that is still differentiable.

Such class-conditional VAEs can be used in any situation where unlabelled data is abundant but labels are scarce.
The discriminator's loss is optimised as part of the VAE's loss function, which means adding more unlabelled data can improve the accuracy of the discriminator.
The approach was developed and tested on the MNIST dataset, which consists of 28x28 grayscale images - a very simple dataset by today's standards.
Some reports have emerged that fail to reproduce this success on more complex datasets such as CIFAR-10, being outperformed even by PCA \cite{vae_bad}.

One related approach is offered by \cite{towards}, where a class-conditional VAE is used to generate synthetic data for a discriminator network, which has reportedly a higher accuracy in semisupervised setting than the approach described just above.
They use it for conditional text generation, but this approach of using the class-conditional VAE to synthesise training data to train a discriminator is applicable to any input modality.
In fact the general approach to ``dreaming up'' new training samples in the \textit{sleep phase} and training the classifier in the \textit{wake phase} was introduced already in \cite{wake_sleep}.
The current author has implemented this model and open sourced it\footnote{https://github.com/mattiasarro/seq2seq-cvae-tensorflow}.

\subsection{Generative Adversarial Networks}

Generative adversarial networks (GANs) are an interesting approach capable of synthesising realistic data, but also useful for learning good representations of data.
GANs have been most successful for image synthesis, but are in fact applicable to any kind of input data including text \cite{textgan} and even mixed data types such as e-commerce orders \cite{ecomgan}.
In the GAN setting, there are two networks: a generator that tries to produce realistic synthetic samples, and a discriminator whose goal is to distinguish between synthetic and actual samples.
% The networks are trained as a min-max optimisation problem where the generator is optimised to ``fool'' the discriminator by producing samples that appear to come from the true distribution, and the discriminator is optimised to  determine which samples are synthetic.
Given that the training is stable enough, both the generator's and the discriminator's performance improves with time, resulting in more realistic synthetic samples, as opposed to the relative blurry images produced by VAEs.
The discriminator needs to learn good representations of the input to  accurately distinguish which samples come from the true distribution, and which samples are synthetic; as with many CNNs for computer vision, these representations can be useful for other tasks as well.

It has been shown that  linear models  from the embeddings learned by the discriminator outperform other unsupervised feature learning techniques such as k-means \cite{dcgan}.
At the time of writing, variants of Wasserian GANs (WGANs) achieve the best performance by improving training stability and preventing the generator from generating samples from a limited number of modes; this is achieved by minimising the Wasserian distance between the generator's and real data distributions, as opposed to minimising the JS divergence as was common before.
A performant variant of WGAN is CT-GAN, which adds a regularisation term to enforce a Lipschitz continuity condition over the manifold of the real data \cite{ctgan}.

\section{Combining Models}
\label{bg_ensembling}

This section  introduces common ensembling methods.
Some of these (bagging, boosting)  are expected to consist of weak learners or models from the same model family, while others (committee methods, BIC, stacked generalisation) or more appropriate for ensembles of strong models.

Bootstrap aggregation (a.k.a bagging)   draws several bootstrap samples  from the training data,  trains a separate model per bootstrap sample,  and averages the predictions of these individual models.  This reduces the variance of predictions without increasing its bias and generally leads to  more accurate predictors \cite{bagging}.
Boosting is another common meta-algorithm that iteratively trains an ensemble of weak models, such that the data points that incurred a higher error on the previous ensemble are weighted more heavily, and each new weak model is added to the ensemble with a weight proportional to the weak learners accuracy.
A common boosting algorithm is AdaBoost \cite{adaboost}.

We are interested in ways of combining models that are separately trained and combined into a single predictor.
Simple solutions are voting or (weighted) averaging, where the weights can be found with k-fold cross-validation.
A more appealing approach is stacked generalisation, where a meta-learner is trained on the outputs of the individual models.
This meta-learner could be a relatively simple model, such as logistic regression or a decision tree, and the results of the meta-learner could be therefore quite interpretable.

One orthogonal approach to combining models is joint training of model components.
All models described here are trained using stochastic gradient descent, so in principle it would be possible to learn the parameters of all components (1D CNN, 2D CNN, RNN, DNN, logistic regression) as part of a single optimisation loop.
This would be an engineering challenge due to the different ways these model require their input to be represented and the large size of the model, and optimisation would surely be difficult.
In our case this would be clearly an overkill, but it would be an interesting research problem to examine whether joint training has advantages over ensembling in cases where we have different views of the data (in terms of considering different input dimensions or processing the inputs differently).
As mentioned in the Wide \& Deep paper \cite{wide_deep}, the wide component of the model has to just handle cases where the deep component falls short, and can therefore be simpler than it would need to be in an ensemble; analogously, jointly training different kinds of models neural networks would allow each component to be simpler.

One of the motivations for picking mostly deep neural networks as our ensemble components was their transfer learning capability: they all produce a dense feature vector (embedding) from which separate logistic regressions predict the class.
While it is useful to have separate embeddings for images, text and categorical variables, we would also like to have a compound embedding of each product.
A simple concatenation of all individual embeddings could be a useful (albeit a quite high-dimensional) representation to be used in downstream models.
Alternatively, we could use this concatenation as an input to a neural network, whose goal is to (1) reduce its dimensionality by removing redundant information and (2) predict the output classes from this compressed embedding.
The latter case could act as a kind of a (replacement for) the meta-learner, with the difference that it takes the penultimate (as opposed to the final) layer of each individual model as input.

% https://arxiv.org/abs/0911.0460
% TODO figure of meta-learner and embedding lear

\section{Active Learning}
\label{bg_al} 

This section describes the main approaches to active learning;  details analysis is beyond the scope of this report.


The goal of active learning is to reduce the number of labels needed  to effectively train a machine learning model by  being selective about which data points to label.
The three general scenarios: membership query synthesis, stream-based selective sampling, and pool-based sampling \cite{al_survey}.
In the first scenario, the algorithm synthesises a datapoint and asks a label for it.
In stream-based selective sampling, and instance is sampled and  the model decides (while having access to the data point's features) whether to acquire a label or not.
In pool-based sampling, the model chooses which data point to label from the pool of all unlabelled samples.
Our use case is a version of pool-based sampling, where the algorithm picks out a batch of products to be labelled.
The rest of the section describes different approaches to picking the data points that are expected to help the model learn.

\textbf{Uncertainty sampling} is the most popular choice: if the model is uncertain about a prediction, obtaining a label for that data point would help it differentiate between different targets.
The most common way to quantify uncertainty is by entropy of the model's predictions:

\begin{equation}
 x^*_H = \argmax_x - \sum_{i} P_\theta(y_i|x) \log P_\theta(y_i|x),
\end{equation}

where $x^*_H$ corresponds to the most informative instance according the entropy ($H$) measure, and $y_i$ corresponds to each class.
% This measure can be used for both multi-label and multi-class problems, and is therefore preferred.

\textbf{Query by committee} uses a number of models trained on the labelled data which all make a prediction on the unlabelled data.
Data points with highest disagreement are expected to be most informative, as by definition a larger number of models would have to be wrong in their predictions.
The models in the committee do not have to be of different type like is our case, but could be e.g. a set of linear classifiers where each committee member just has different parameter values, yet each member would have to be consistent with the current set of labels (not contradict it with its predictions).
Disagreement between committee members can be quantified with vote entropy \cite{vote_entropy}:

\begin{equation}
 x^*_{VE} = \argmax_x - \sum_{i} \frac{V(y_i)}{C}  \log \frac{V(y_i)}{C},
\end{equation}

where $V(y_i)$ is the number of votes given to class $i$ and $C$ is committee size.
Alternatively, KL divergence between each committee member's predictions and the consensus $P_\zeta$ (average prediction of committee members) could be used:

\begin{align}
 x^*_{KL} &= \argmax_x \frac{1}{C} \sum_{i} \infdiv{P_{\theta^{(C)}}}{P_\zeta}\\
 P_\zeta(y_i | x) &= \frac{1}{C} \sum_{c=1}^C P_{\theta^{(C)}}(y_i|x)\\
 \infdiv{P_{\theta^{(C)}}}{P_\zeta} &= \sum_i  P_{\theta^{(C)}}(y_i|x) \log \frac{ P_{\theta^{(C)}}(y_i|x) }{ P_{\zeta}(y_i|x) }
\end{align}

\textbf{Expected model change} calculates how much a model would change if we knew its label.
In gradient-based learning algorithms it is possible to compute the L2 norm of the gradient vector for each combination of labelings of the unlabelled product \cite{model_change}, which is a direct measure of how much the model would change.

\textbf{Expected error reduction} considers how much the test set error is likely to decrease with the acquisition of a given label.
This is done by re-training the model once per each combination of the label values and observing how either the risk or expected log loss of the unlabelled data points decreases.
This is not a suitable use case for deep networks, which are trained over several epochs of the data; there is no efficient way to incrementally train the model, and when re-training with a single additional label, the change in the model is probably higher from the stochasticity in the training procedure rather than the additional label.
In any case, this method is very expensive computationally.

\textbf{Variance reduction} tries to pick data points which are expected to decrease the variance in predictions.
To do that, the Fisher information of the model parameters should be maximised.
Unfortunately this involves inverting a $K \times K$ covariance matrix, where $K$ is the number of parameters in the model.
This is impractical for deep networks with millions of parameters.

\textbf{Information density} measures consider data points that are not just uncertain but also representative of the underlying distribution.
This avoids the problem with many other approaches that ask labels for samples that are controversial, but could otherwise be outliers and not the most useful for good generalisation.
In this framework, a base informativeness measure $\phi_A(x)$ such as uncertainty or disagreement is weighted by the average distance of the data point $x$ to other data points in the distribution \cite{inf_dens}:


\begin{equation}
 x^*_{ID} = \argmax_x \phi_A(x) \times \Bigg( \frac{1}{U} \sum_{u=1}^U sim(x, x^{(u)}) \Bigg)^\beta,
\end{equation}

where $sim$ is some similarity function and $\beta$ controls the importance of the density term.

\section{Multi-Objective Learning}



\section{General Practices in Machine Learning}

We briefly list some techniques and approaches that could be used for many kinds of machine learning problems.
Some of these will be used in our experiments, yet some may not be pragmatic due to long train times and time constraints.

Bootstrap sampling, where data points are sampled from training data with replacement, can be used to repeatedly train the same model, and to observe the variance of its predictions.
Bumping can be used to train several models (of the same family) on bootstrap samples to move around in the model space, and will pick the model that best fits the training data.
This helps it explore a wider selection of models, but given that the original training data often also included as one of the bootstrap samples, this method  can still pick the model original model if it happens to have the best accuracy.

There are some versatile methods that can help regularise a neural network (deep or shallow), or to speed up convergence by improving  gradient updates.
L2  regularisation should always be used to decrease moral complexity and impose prior on the model parameters, which  implies that very high and very low values are unlikely.
Dropout can mitigate overfitting by preventing neurons from co-adapting, encouraging each to learn representations that are useful independently \cite{dropout}; this has been interpreted as training and exponential (in the number of parameters) number of models and averaging their predictions.

gradient clipping
layer norm
Batch normalisation is known to stabilise training and hence speed up convergence  by normalising each input to have unit variance and zero mean \cite{batch_norm}.
