\chapter{Background}


\section{Input Representations}
\label{data_rep}

\subsection{Categorical Input}

The simplest option for representing categorical input is 1-hot encoding, where input has as many dimensions as there are distinct categorical values (vocabulary size); a single dimension is set to 1, with all other dimensions set to 0.
This is a straightforward representation: for a simple model such as logistic regression, we can clearly interpret the model parameters and see how the presence or absence of a given category increases or decreases the likelihood of a given output.
However, in cases where vocabulary sizes get  large, and the number of outputs (the number of units in the next layer in a deep network,  or the number of output units  in a shallow one) increases, this kind of encoding can really blow up the number of parameters of the model.
 this has many negative consequences: more memory, more required train data - curse of dimensionality

An alternative is to use  random embeddings:  dense, low-dimensional representations of a high-dimensional vectors.
It has been shown in compressed sensing literature that if a high-dimensional signal in effect lies on a low-dimensional manifold, then the original signal can be reconstructed from a small number of linear measurements \cite{compressive_sensing2}.
This has a useful implication for machine learning: categorical variables with large vocabularies of size \textit{d}  can be  represented with  random embedding vectors of size $M = \log(d)$.
This because it is possible to reconstruct any d-dimensional k-sparse signal using at most $k  log{\frac{d}{k}}$ dimensional vectors \cite{compressive_sensing1}, and 1-hot vectors are 1-sparse (only one dimension is nonzero).

An intuitive example is given in \cite{compressive_sensing3} about natural images: a 1-million-pixel image could have roughly 20,000 edges\footnote{technically: wavelet coefficients with a significant power, which roughly corresponds to a superimposition of edges}, i.e. it lies in a roughly 20,000-dimensional manifold from which the original image can be constructed with little loss in quality.
The 1-million-dimensional image could be projected into a M-dimensional space ``by taking M measurements of the image, where each measurement consists of a weighted sum of all the pixel intensities, and allowing the weights themselves to be chosen randomly (for example, drawn independently from a Gaussian distribution)''\cite{compressive_sensing3}.
The projection these random because the weights for the sum of pixel intensities are chosen randomly.
Why the projections need to be random is motivated by another intuitive example:  when light is shined on a 3-dimensional wireframe, its shadow is a 2-dimensional projection of it.
The projection could lose some important information about the regional object if the direction of the light source is chosen poorly,  however random directions are likely to result in projections where every link of the wire will have a corresponding nonzero length of shadow.

\subsection{Text Input}

The simplest way to represent  text is bag words (\textbf{BoW}),  which  is simply the histogram of word occurrences  in the text.
BoW  representations give equal weight to each of the words in the text, and the model has to learn the relative importance of each of these.
A commonly used representation in information retrieval (IR) is \textbf{TF-IDF},  which stands for ``term frequency -  inverse document frequency''.
TF-IDF  multiplies the term frequency (number of times the word occurred in the text) with its inverse document frequency ( the inverse of how often a occurs in all documents).
This has the effect of giving lower scores for common words, and higher scores for words which appear often in a document but not too frequently in the whole corpus.
Another common representation is bag of n-grams (\textbf{BonG}), which  is a histogram of character n-grams.

The above representations are sparse: there are vector representations grow linearly with the vocabulary size.
Text can also be  represented as a sequence of \textbf{word embeddings}.
Words and betting can either be random vectors or representations  learned with word cooccurrence algorithms such as word2vec \cite{} and GloVe \cite{};  these representations can remain fixed throughout the learning procedure, or updated as part of the stochastic gradient descent (SGD) when applicable.
A simple yet surprisingly powerful way to represent text is to average the word embedding contained in it \cite{}.
The  average could also be weighted by the TF-IDF score of each word\footnote{the author is unaware whether this has been tried before, but it seems a promising approach}.

More complex methods use recurrent neural networks (RNNs) to combine a sequence of word embeddings  into a \textbf{fixed length vector},  which are described in more detail in section \ref{rnn}.
These kinds of models are good at  disambiguating the potentially many meanings a word might have.
Exactly what gets persisted in the final  vector representation of text depends on how the model is trained - two RNNs  trained on different tasks (such as sentiment analysis and named entity recognition)  would need to store different information in its hidden state to successfully  perform their relevant tasks,  hence the encoding for the same sentence would be different for either model.
Still, an RNN  that is trained on one task could provide useful features for another.
Encoding text as fixed length vectors using RNNs has been interpreted as compressed sensing \cite{compressed_sensing_rnn}, with such vectors being ``provably at least as powerful on classification tasks, up to small error, as a linear classifier over BonG vectors''.

\subsection{Image Input}

Images can be represented as dense 3-dimensional tensor.
A 28x28 RGB image could be represented as a tensor with shape [28, 28, 3],  with the final dimension corresponding to the green, red, and blue intensity values at a given x/y coordinate.
These intensities  are usually in the range 0 ... 255, but are normalised before input to machine learning model.
Similarly to RNNs  encoding a sequence of words to a vector, 2D convolutional neural networks (2D CNNs)  can be used to extract dense feature vectors from raw images (image embedding),  further discussed in section \ref{image_models}.

\section{Models Considered}
\label{models_considered}

The following sections describes a selection of models that could be used for our classification task.
This is by no means an exhaustive list, nor is it a selection that is expected to have the highest predictive performance individually.
These models will later become part of an ensemble, where diversity matters much more than the performance of any individual model.
Engineering  considerations and time constraints also influence the selection of models: we did not want to spend time implementing models from scratch, or to use many frameworks for training models.
TensorFlow  is a widely used machine learning library with a good selection of open source models, in particular a selection of pre-trained models for computer vision and NLP, and has arguably the most mature deployment ecosystem.
Therefore models that did not have an open source TensorFlow implementation were automatically excluded.

Section \ref{general_models} describes models that can take categorical and text inputs, section \ref{text_models} looks at some neural models that take only text inputs, and section \ref{image_models} describes 2D CNNs that can classify or extract features from images.
In all cases the models are described in terms of how they could be used for solving the problem of multi-label classification.

\subsection{General Models}
\label{general_models}



\subsubsection{Logistic Regression}

Logistic regression is a linear, discriminative classifier that is often the baseline model of choice, since it is both interpretable and efficient to train.
There are methods that take time linear in the number of non-zeros in the dataset, which is the smallest amount possible, and it can be made to handle non-linear decision boundaries by using kernels \cite{murphy}.
Our case is the binomial logistic regression, with a separate logistic regression model for each output class.
This is shown formally in eq  \ref{logistic}, where where \textit{x} and \textit{w} correspond to the input and weight vectors, respectively.
We follow the notational trick where the first element of \textit{x} is always 1, and the first element of \textit{w} corresponds to the bias term.
The loss function of this model as well as how it is optimised is described in section \ref{loss}.

\begin{equation}
\label{logistic}
p(y|x,w)=\mathrm{Ber}(y|\mathrm{sigm}(w^Tx))
\end{equation}

\subsubsection{Feedforward Neural Network}

Feedforward neural networks (also called deep feedforward networks, multi-layer perceptrons) have been described as function approximator, whose goal is to approximate some functions $f^*$ \cite{dlb}.
We expect some familiarity with neural networks from the reader; for an excellent overview of the various use cases and models of deep learning, refer to.
In our case, the input is some representation of categorical and text input: sparse BoW or TF-IDF of text, 1-hot encoded or random embedding vectors of categorical variables, or embedding is extracted from text or images using pre-trained recurrent or convolution on neural networks.
The hidden layers of our network are homogenous: they use the same activation function (ReLu, sigmoid, or tanh) and have the same number of units;  activation functions and the number of units in hidden layers are determined through hyperparameter tuning.
The output layer uses sigmoid activation function  and has the same number of units as there are classes.

\subsubsection{Loss Function and Optimizers}
\label{loss}

Logistic regression and neural networks are optimised with some variant of gradient descent.
For our task the loss function binary cross-entropy between the training data and the model distribution, with the loss value averaged across all classes.
This is given in eq. \ref{xentropy}, where $\theta$ corresponds to the model parameters such as the weights and biases of the model, $C$ corresponds to the number of classes, and $P(c=1|x, \theta)$ gives the predicted probability that the item belongs to class $c$ given the feature vector $x$.

\begin{align}
  \label{xentropy}
  NLL(\theta) &= \sum\limits_{c=1}^C \sum\limits_{i=1}^N\left[c_i\log P(c=1|x, \theta) + (1-c_i)\log(P(c=0|x, \theta) )\right]
\end{align}

We are minimising the negative log likelihood (NLL) rather than maximising the product of likelihoods.
Multiplying large numbers of probabilities could result in numerical underflow and rounding errors, therefore it is pragmatic to work with sums of log probabilities instead.
The Adam \cite{adam} optimiser works very well with default hyperparameters, and is therefore used for all stochastic gradient descent updates.

\subsubsection{Tree-Structured Models}


\subsection{Text Models}
\label{text_models}

\subsubsection{1D Convolutional Neural Networks}

A simple convolutional architecture for text classification is described in \cite{1dcnn}.
Pre-trained $k$-dimensional word vectors are concatenated to represent a sentence, and a filter is $w \in R^{hk}$ is applied to a window of $h$ words at each possible window of words in a sentence; this gives a single variable-length feature vector representing the sentence.
Several such filters are learned (each with potentially a different width $h$), and max-over-time pooling is applied to these feature maps; this gives a vector of the highest activations from each filter, which is then passed to a fully-connected softmax layer for final classification (in the case of multi-class classification).
For multi-label classification, the final layer would be a fully-connected layer of sigmoid activations.

This simple architecture should be able to predict output classes reasonably.
Each filter can indicate the presence of a a sequence of $h$ words; max-pooling discards information about where exactly in the text it appeared, and ensures the output is of a fixed length.
The same filter $w$ is used across all possible word windows in the sentence, which can be seen as a form of parameter sharing (and as an infinitely strong prior over the parameters of the model \cite{dlb}), and enables processing variable-length sequences.
The relatively small number of shared parameters requires less training data than an equivalent fully-connected architecture.
Using pre-trained word embeddings further simplifies the task, as these already carry some information about the meaning or syntactic role of words.

More sophisticated architectures can be built using 1D CNNs, although not used in our experiments.
Most notably, several layers of convolutions (each optionally followed by pooling) can be stacked, where the following layer works on the feature maps output by the previous layer.
The model described above uses a stride and dilation of 1, but multi-layer architectures where the higher layers have use exponentially increasing dilation are able to expand the receptive field of the model, and as a result aggregate ``contextual information from multiple scales'' \cite{dilated}.
Dilated convolutions were beneficial for semantic segmentation of images with 2D CNNs, but this increased receptive field has been useful for sequence-to-sequence models in NLP \cite{dilated_decoder}.

\subsubsection{Recurrent Neural Networks}
\label{rnn}

Recurrent neural networks (RNNs) are a family of models for processing sequential data.
The sequence of inputs $x^{(1)} \dots x^{(t)}$ in our case are vectors of word embeddings, but there are other options for input representations (e.g. sequence of 1-hot encoded tokens, or vectors representing a multivariate time series at a given time step $t$).
Vanilla RNNs maintain a hidden state vector $h$ which contains some information about the sequence it has seen so far, and is calculated from the input at the current time step $t$, and the previous hidden state:

\begin{equation}
  h^{(t)} = f(h^{(t-1)},x^{(t)})
\end{equation}

Vanilla RNNs suffer from the vanishing and exploding gradient problem in long sequences and are notoriously difficult to train.
Probably the most widely used variant, the long short-term memory networks (LSTM) \cite{lstm} overcome this limitation by augmenting the network with a memory cell $c$; there is a learned gating mechanism that controls what part of (and the extent to which) the cell is forgotten, what gets persisted in the cell state, and what gets output at a given time step.
An intuitive overview of the gating mechanisms is given by \cite{colah} and \cite{vanishing} describes well how this helps mitigate vanishing gradients.
A simpler gating mechanism is offered by gated recurrent units (GRU) \cite{gru}, which has fewer parameters and works well on simpler tasks.

We are interested in using RNNs as text encoders, though they are often used for sequence to sequence modelling, or for predicting an output at each time step.
In the simplest case, the concatenation of the cell state $c$ and the hidden state $h$ at the last time step could be used as a representation of the sequence.

\subsection{Image Models}
\label{image_models}

2D CNNs are important historically - excellent results in computer vision revived interest and funding in deep learning - but also applicable to a wide range of problems.
Computer vision CNNs tend to be complex models with millions of parameters and dozens (or even hundreds) of layers.
It is very compute-intensive and time-consuming to search for new architectures, and the analysis of these is beyond the scope of this work.
Our goal is to use a CNN that has a reasonable performance without incurring unreasonable overhead.

The general architecture of a 2D CNN is as follows.
Patches of feature extractors are convolved over the x and y axis of the image; these are often small, e.g. 3x3x$c$ patches that span all the $c$ channels of the image ($c=3$ for RGB images), and convolution could be strided.
Former models used larger patch sizes, but stacking several smaller patches could achieve similar receptive fields while allowing for a larger number of non-linearities for the same number of parameters.
One or more layers of convolutions would be followed by a (often max) pooling layer.
Computer vision architectures tend to re-use the same sub-structure repeatedly: either the same kinds of convolutions followed by pooling, or the ``network in a network'' structure popularised by GoogLeNet \cite{googlenet}.
After a number of such repeated structures, there are normally a few fully-connected layers followed by a softmax layer.
In our case, the final layer would be a fully-connected layer of sigmoid activations.

Transfer learning has shown to be particularly useful in computer vision.
Successful computer vision models require a notoriously large training dataset, which would be hard to come by for every distinct computer vision task.
Instead, models are often initialised to the values of a model that has been trained on a large dataset such as Imagenet \cite{}, excluding only the last layer(s) of the model that has learned features that are specific to the original problem.
Representations learned at the lower layers are remarkably  universal and useful for other tasks.
Therefore pre-trained 2D CNNs can be used as feature extractors for various other tasks,  or the models could be fine-tuned to learn representations that are even more useful to the new task.

\section{Unsupervised and Semisupervised Learning}
\label{unsup}


\section{Combining Models}
\label{bg_ensembling}


\section{Active Learning}
\label{bg_al}


\section{Approaches for Visual Similarity}
\label{bg_sim}
