\chapter{Discussion}

In this section we analyse the experiments that were described in the previous section.
Since many different topics were explored, we organise the discussion into subsections.

\section{Visual Similarity}

Although this assessment is subjective, the results from the approximate nearest neighbour were strikingly good; this is the opinion of many people at the client company that saw a demo of it.
The model did not require fine-tuning, contrary to the initial hypothesis.
The biggest challenge here is to find a way to evaluate visual similarity performance; although there are various algorithms for tweaking similarity and evaluating it on a test set, creating such a dataset would have been prohibitively expensive.
Given that vanilla pre-trained CNNs produce good embeddings for visual similarity, it is sensible to focus one's efforts on the scalability and productionalisation aspect of this problem.
Still, two relatively easy directions could yet be explored: the model architecture and the layer from which embeddings are extracted.
Our model of choice was Inception V3, which had a good accuracy to memory footprint ratio - with respect to the original ImageNet challenge.
However it has been shown that models that are well tuned for a given classification task are not necessarily the best feature extractors; in fact, ResNets are currently the best at this \cite{img_feature_extract}.
The other decision of of using the penultimate layer of the CNN as a feature extractor is also a common one.
Often the transfer learning task is classification, for which the higher-level fully-connected layers would provide good high-level semantic features.
In our case, extracting features from the first fully-connected layer or even from the raw convolutional feature maps at a lower layer might have given us similarity that is highly sensitive to certain visual patterns and shapes - which could be deployed to certain categories where such sensitivity is required.

As already discussed, the tokenised versions of feature vectors had less spectacular results.
While appealing as a way to combine visual and textual similarity, there is not theoretical justification for this approach.
Note that the original paper \ref{vec_fulltext} extracted dense features from text, and only hypothetically proposed this approach to be applied on images.
Perhaps the inherent discrete nature of language (sentences, words, topics as opposed to a continuous 2D space)  makes it more amenable to such tokenisation.

\section{Individual Models \& Hyperparameters}

Choosing a relatively complex model such as Wide \& Deep as a baseline model can be criticised: a baseline should be simple and interpretable.
One of the justifications was the ability to turn some part of the model off, as removing all deep input columns would have resulted in a linear model and vice versa; eventually it was easier to have separate model types (deep, linear, wide \& deep).
The other intuition was drawn from models with skip connections such as ResNets \cite{resnet} and DenseNets \cite{densenet}.
The assumption was that linear models are relatively good at predicting the output, as the relevant word tokens would be present in the textual fields of many products; for a deep model to use this raw knowledge, it would need to learn an identity function from the input of the layer to the output, which can be a hard problem with some loss functions.
Therefore, a linear layer with the same input as the deep layer could be justified to simplify learning for such simple cases, though not necessarily as the first ``baseline''.
Overall Wide \& Deep performed very well, though it is hard to compare its PR AUC to the other models, which were trained and evaluated on twice the amount of data.
When its performance degraded once it was trained on a new dataset (that most likely had mixed up image embeddings of some products), its drawback became apparent: we had no clue which of the input features were most responsible for good or bad performance.

Training a large selection of deep and shallow models was a good way to get a sense of the capabilities of these models.
The performance of linear models was lower than expected, but this is probably not a fair comparison, as the models were trained on the same sets of hyperparameters that were at least in part chosen to be suitable for deep models.
In general the tools used (TensorFlow, Adam optimiser) may not be optimal for learning linear models, but considering the large difference of PR AUC between the two, there seem to be benefits to using deeper models.
Of course the biggest problem in interpreting the PR AUC scores of the rule-based objective is that its labels were generated using a relatively simple automatic procedure which only considers text as its input.
Therefore a high PR AUC score shows us only how well the model was able to reproduce the behaviour of the rule-based system, but we are after good generalisation and consistent multi-objective performance.
We consider the classification problem to be a quite easy optimisation problem, however the power of more complex models might be better served when we are adding more training objectives (e.g. colour, style, conversion of products).

Some patterns emerge from training this selection of models.
For linear models, 1-hot and k-hot encoding always outperforms embedding inputs.
Deep models pre-trained on other objectives are quite consistent in their generalisation, producing strikingly similar PR AUC and recall scores whether it takes the title, description or image as input.
Examining the true/false positives/negatives of individual classes reveals that these models are relatively good at predicting certain classes, yet fail to differentiate between more specific ones.

It would be interesting to compare the performance of these models with random forests and 1D CNNs; fine-tuning of pre-trained models would also be an interesting comparison, as would models trained on character n-gram representations or TF-IDF-weighted embeddings.
Our embedding models could further be improved by not limiting the number of unique tokens so aggressively; as embedding based models have shown to perform better than 1-hot encoding in deep models, we could expand the vocabulary sizes without increasing input dimensionality.

Hyperparameter tuning using Bayesian optimisation was not as successful as expected.
In the first round where the Wide \& Deep model was tuned, the final metric value was only marginally higher than PR AUC of manually chosen hyperparameters, yet the model complexity was far greater.
A simpler model with comparable accuracy is more likely to generalise better, therefore most of the following deep models used 4 layers.
In the second tuning round, the model failed to learn in any instance; this is probably because the large range supplied for the ``L2 scale'' parameter.
An examination of the individual models that were trained during tuning showed that L2 decay overpowered the training loss and resulted in a very strict model that nearly always predicted the negative class.
The rate weight decay scaling parameter was kept fixed at 0.0005 for future runs based on analogous values used in literature; there is also some evidence that one should specify a custom scheme of weighting the L2 loss with adaptive optimisers such as Adam \cite{fix_adam}.

Given that we were training models with high capacity, it would also be useful to know the variance of the outcome if the model is trained several times by doing k-fold cross-validation.
Due to the large dataset size and time limitation this was not tried formally, however these models were trained across dozens of different data pre-processing runs during which the train/validation split was different.
There was not much variability within even the high-capacity models, which often converged at a high AUC PR score.

\section{Multi-Objective Training}

Even though the rule-based objective gave good PR AUC scores, the model was not deployable as is, since it severely under-predicted all nontrivial categories.
As one might expect, training for multiple objectives has a higher variance in the gradients.
This was exacerbated by the limitations of the tf.data APIs, which did not enable to efficiently alternate between the training objectives precisely.
Probably the variability could be reduced if each batch had a consistent number of products from each objective.
For more thorough experimentation, one should probably fall back to manually feeding data batch-by-batch, as was the standard in earlier versions of TensorFlow.

Although Adam was a versatile optimiser across the different deep and shallow architectures for the rule-based objective, updates to the parameters of each top layer becomes less frequent in the multi-objective case.
As Adam learns to update less frequently updated variables more heavily, this is probably the cause of a wide range of issues described in section \ref{multiobj}.
Note that it is not formally confirmed that the peculiarities are caused by adaptive learning rates.
Experimenting with a simpler optimiser and outputting per-step statistics about how many data points from each objective ended up in each bach would help further understand the shape of the loss functions.
For example, currently the best guess to the peculiar shape of the rule-based training loss (blue line in figure \ref{pretrain}) in a multi-objective training setting is that the large jumps are caused by a large batch of data points from the exclusive objective which has had few updates in a while; the medium-variance periods that follow such jumps (e.g. steps 40k - 80k) also contain data from that objective; the gradually decreasing plateaus (e.g. steps 80k - 120k and 170k - 200k) are periods where the model is trained only on the rule-based objective, which is considerably more stable.

It is satisfying that a multi-objective training scheme outperforms a method trained on just exclusive labels.
Though the exclusivised objective at times exceeds multi-objective training in terms of accuracy, the latter is preferred due to its stability and predictability; also, this scheme will be useful once we add additional training objectives.
The 60\% on the exclusive objective is not ideal.
This is achieved with roughly 6000 manual labels, which is far from the millions of labels deep neural networks usually require.
Probably the biggest gains will come from re-organising the category tree further so that there are no ambiguous classes, and that all the ambiguous cases (e.g. ``Leather Jackets'') are handled as a combination of an exclusive class (``Jacket'') and independent feature detector (``Material: leather'').
The remaining gains will come from active labelling, which should find the uncertain cases, or by manually adding labels to categories with a high error rate.
An orthogonal approach is to train using the Wasserstein loss, which would be better at handling ambiguous and noisy labels; in this case, we might use the Wasserstein distance between the predicted and actual labels as the evaluation metric as well, as accuracy would still paint a pessimistic picture when in reality it is acceptable to classify into a semantically nearby class.

A separate question is how to phase out the old rule-based system that the client company wishes to replace.
As the rule-based system would stop producing labels, any new products would be unlabelled according to it.
This would happen more often than one might think due to the way the data import process happens.
The most likely solution is to create an interface for mass-labelling of products: present the labeller pages with hundreds of products it thinks belong to a category, and have them confirm the page or pick out incorrect predictions.
This way we create a large number of high-quality labels with relatively little effort.

\section{Active Learning}

Preliminary results indicate that uncertainty sampling is effective at identifying products that might lack a category or sufficient labels in the training data, but it tends to sample products that are very much alike.
Therefore this strategy may not be very well suited for such a ``batch-active'' learning scenario as was presented.
A workaround might be a series of smaller training rounds.
Identifying unseen categories is a useful feature, though an analysis of validation error might produce similar results.
