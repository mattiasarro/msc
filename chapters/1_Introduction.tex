\chapter{Introduction}

% \chapterprecishere{''Begin at the beginning'', the King said gravely,'' and go on till you come to the end: then stop.'' \par\raggedleft --- \textup{Lewis Carroll}, Alice in Wonderland}

% ML is successful especially DL
Machine learning (ML) has recently become a popular research area, and is increasingly applied in industry.
A lot of this newfound interest and hype is directed at  neural networks and deep learning.
This focus is not unfounded - approaches based on deep networks continue to break benchmarks in  core machine learning research areas such as computer vision \cite{sota_img1, sota_img2, sota_img3}, speech recognition and synthesis \cite{sota_asr, wavenet}, many natural language processing tasks \cite{sota_nlp1, sota_nlp2, sota_nlp3}, game play \cite{alphago, deepstack}, and even enabling novel applications such as style transfer \cite{style_transfer} and content generation \cite{gan}.
% Deep learning has also revolutionised reinforcement learning, achieving superhuman performance in complex games and driving vehicles in real-world situations.
% There are even limited results in  beating human at highly uncertain games with various actors such as [Texas Holdem] poker [cite].

% why DL is useful and flexible
Artificial neural networks consist of layers of transformations that map multidimensional inputs to (usually multidimensional or structured) outputs.
A single-layer neural network is a linear transformation of the inputs; each additional layer with a non-linear activation function enables the network to partition the output space and hence approximate more complex functions.
Complex models are prone to overfitting and require more training data and heavier regularisation, yet deep models are somewhat unique in that their performance continues to increase with the size of the dataset, while the benefits of more data taper off for other models.

The naive conclusion is to use deep learning only when labeled data is abundant, but the depth of a model is not a binary ``deep vs shallow'' decision - one can start out with a shallow model and increase model complexity to see the effects on performance and generalisation.
Logistic regression might well be the most appropriate model for a classification problem, but this is often not obvious up front, so it is beneficial to build models in a framework that also supports deep models.
In addition to easy experimentation with model architectures, it is easy to do transfer learning with deep models on popular platforms: image or text models pretrained on large datasets can be utilised to increase product classification performance, and representations learned for classification could in turn be used by a downstream product recommender system.
Datasets with abundant unlabeled data can benefit from unsupervised and semisupervised learning using deep generative models.

%  balancing between predictive performance, label complexity, transfer learning capabilities
% It is not immediately obvious  which kind of model should one use for a given task and dataset.
% A data scientist can consider the following factors:  how many labelled and unlabelled data points do we have,  how much do we value predictive performance, interpretability, and whether we want to do some transfer learning or joint training with the models.
% Labelling is often expensive, so in many real world use cases a lower label complexity (number of labels needed to obtain the desired accuracy) is preferred over slightly better performance.
% Deep learning seems to have a disadvantage in this aspect, but as we see in section [ref] in cases where unlabelled data is also abundant, semi-supervised and generative models can overcome low label complexity while increasing computation time.
% In cases where the ability of neural networks to learn features that can be used in downstream models (e.g. features learned for classification could later be used as part of a recommended system) this increased computation and engineering complexity might be justifiable.

% In this work we are concerned with product classification and similarity, but another project might have vastly different priorities and expectations of the above aspects.
% In e-commerce classification, low label complexity is important and some inaccuracy is acceptable; interpretability and computational resources are not much of a concern.
% For product similarity, the capability to do transfer learning is critical, which compensates for the increased demands on engineering.


Each project has different requirements on predictive performance, label complexity, interpretability, computational resources and engineering challenges.
Neural networks are a good fit for this project due to their amenability to transfer learning and flexibility in handling multi-modal inputs and multi-task objectives.
As neural networks are notoriously data-hungry, the question immediately becomes - how to do it without providing many labels.

% we explore individual models, ensembling, active learning
% In this thesis, we explore three orthogonal ways of efficiently learning on a proprietary dataset for product classification, where initial labels are abundant but noisy. We first evaluate different kinds of models (shallow, deep, tree-structured, convolutional, recurrent) that are trained on different modalities / input dimensions (image, text, categorical, numerical) of the same data. After determining the performance of these baseline models, the strongest models are trained as an ensemble that outperforms each individual baseline model. Finally we fine-tune the ensemble via an active learning strategy described in section [ref], where a combination of uncertainty and disagreement sampling determines a batch of products to be labeled for the next training iteration. This overcomes the noisiness and incompleteness of the initial labels without requiring much manual labeling.

\section{Problem}

% should this be in the problen statement?
The client company gathers data from various affiliate networks (that in turn get their data from various retailers) and displays it on their website.
There are millions of products belonging to roughly 1300 categories, and categories follow the usual nested tree structure.
The incoming data is noisy and inconsistent: what kind of data is stored in which column varies across affiliate networks, across retailers within an affiliate network, and the data within a retailer can have lots of missing values, noisy text, missing images, etc.
There is currently a rule-based system for assigning products to categories: all products matching a condition (e.g. title contains the word ``trousers'') will be assigned to that category, i.e. categories are not mutually exclusive.
This way of categorising products works relatively well on some categories, but such a rule-based system has several drawbacks: these rules are cumbersome to define, their evaluation is manual, they fail to match a large fraction of products that in principle should be in a given category, it is hard to trace back the rule that caused a false positive, and such rules are limited to textual data.

The client needs a classification system to replace the old way of categorising products.
The predicted outputs can either be a set of independent binary classifications, or the category structure has to be re-organised to ensure classes are mutually exclusive.
The system should be able to learn from the output of the old system, and if possible produce models that can be used in downstream tasks such as recommend systems and product similarity models.
The highest priority is low label complexity, beating requirements for high accuracy and interpretability.
The system should be robust to  noisy inputs; data preprocessing should not consider the idiosyncrasies of each affiliate network.
There is an additional feature the client requires: given a product image, the visitor should be shown products that are visually similar.

\section{Purpose}

The academic purpose of this work is to (1) assess the relative strengths of different kinds of models and their combinations, and (2) to determine whether an active labelling strategy reduces label complexity on a real-world dataset. Analogously, the commercial purpose is to (1) obtain a model with powerful predictive capabilities, and to (2) reduce costs by using an efficient labelling strategy, and (3) obtain a high-quality product similarity score.

\section{Goals}

The goals of the work, in chronological order, is to:

\begin{itemize}
  \item Use a pre-trained 2-dimensional convolutional neural network (2D CNN) to extract features for an approximate nearest-neighbour search of visually similar products.
  \item Build an interface for subjectively evaluating the visual similarity algorithm.
  \item Train  baseline model to reproduce the behaviour of the rule-based system.
  \item Train and evaluate a number of different models on the rule-based labels.
  \item
    Define a subset of the 1300 independent categories to be exclusive (non-overlapping).
    Build an interface for assigning \textbf{exclusive labels} to products either through \textbf{uncertainty} or \textbf{random} sampling.\footnote{The final version of this interface was built by an employee of the client company, though earlier versions for displaying the results of classification / item similarity were implemented by the author.}
  % \item Train a selection of models that had good performance as an ensemble, preferring model diversity over good performance. Evaluate this model on the rule-based test set as well as the ground truth dataset.
  \item Implement the active labelling mechanism defined in \ref{exp_al} and train a small selection of well-performing models in the following 6 settings:
    \begin{itemize}
      \item One training objective: \textbf{exclusive}.
      \item Two training objectives: \textbf{exclusive} and \textbf{rule-based}. In addition to human-assigned exclusive labels, the model is trained on the original 1300 independent categories assigned by the rule-based system. \footnote{We call this the rule-based rather than independent objective, as we are planning to train with independent training targets other than the rule-based labels at a later stage in the project.}
      \item Both training objectives are trained in three settings: exclusive labels are either limited to random or uncertainty sampling (to determine the efficacy of active learning), or not limited (labels from random and uncertainty sampling are combined to have the largest training set).
    \end{itemize}
  \item Document the results as well as the technical architecture and workflow.
\end{itemize}

\section{Hypotheses}

The following hypotheses were postulated prior to running most experiments\footnote{the Wide\&Deep baseline model had been trained on rule-based labels}:

\begin{enumerate}
  \item Pre-trained 2D CNNs perform reasonably for visual item similarity, but fine-tuning might be needed.
  \item Linear models are relatively good at predicting higher-level categories, worse at predicting lower-level ones.
  \item Deep models with histogram inputs do not improve substantially on linear models.
  \item Shallow models with embedding inputs generalise better, and deep models with embedding inputs generalise slightly more.
  % \item XGBoost is the best performing model  for textual and categorical data.
  \item Pre-trained 2D CNN are consistently good predictors  of clothing products, yet fail to accurately predict categories such as technology.
  % \item Fine-tuning CNNs  will give little, if any,  performance  improvement due to insufficient training data.
  % \item Training CNNs with DCGANs would obtain better results than fine-tuning CNNs, yet given the strong baseline performance of pre-trained CNNs this is not crucial for a good overall performance.
  % \item Ensembling gives better performance than any individual model.
  % \item A meta-learner with a few layers  obtains better results than ensembling by averaging.

  \item Active learning substantially decreases label complexity, however uncertainty sampling might give poor results on the first round.
\end{enumerate}

\hfill \break
How these hypotheses were evaluated is described in section \ref{evaluation}.

% \section{Work Environment}
% \section{Deployment Environment}

\section{Ethical Consideration}

Given how pervasive  machine learning is becoming, it is crucial that there is discussion about the safety, transparency and bias of machine learning systems.
There have already been cases of black box algorithms being used to make decisions that severely influence a person's life -  predicting reoffending probability  of convicts to determine  whether they are given parole - with terrible results.
The model that was touted to be and objective substitute to a subjective judge has simply learnt that biases inherent to the data (the prejudices and racism of  the judges at the time):  it consistently underestimated reoffending rate of white convicts and overestimated the reoffending rate for black convicts \cite{propub}.
Even though the data did not contain explicit information about race, the algorithm  was capable of implicitly detecting race through some other variable that was correlated with race.

Most data scientists will never build models with such severe consequences, but even more subtle decisions  made autonomously by algorithms  can prolong the  inequalities of our society by consistently favouring certain  characteristics such as race, gender, place of origin, etc.
Prime examples are algorithms that determine whether one gets a mortgage or not, what the premium on their insurance would be, whose CV gets shortlisted for a position, and so on.

The worst case scenario for inaccurate,  opaque or biased product classification  is embarrassment for the client company,  therefore ethical considerations are not of much concern in our case.
It is still worth reminding the reader that machine learning  models always contain some kind of bias:  from the data it uses as input (the way it is gathered, what is gathered),  the way the data is processed, and the kinds of models used.
One way to quantify the fairness of a binary classifier the p\%  rule, which sets bounds on the ratio between the probability of the outcome given the sensitive attribute \cite{quantfair}.
One way to overcome a classifier that seems to rely on some ``nuisance parameter'' (such as gender or race - even when the parameter is removed from the training data) is adversarial training \cite{quantfair} to ensure the model's output distribution does not depend on the sensitive parameter.

A related concern to fairness is privacy: ensuring that a ML model does not leak its training data to an attacker that might orchestrate a large series of queries on the model.
Each individual query might not reveal much, but with a large number of queries the attacker can see how the statistics of the output distribution changes; paired with an external dataset for cross-referencing or knowledge that a data point was added to the training set (e.g. someone answered a questionnaire), it is possible to recover some of the original data (though not necessarily with high fidelity or probability).
A countermeasure is differential privacy, which can be applied to the data sharing process or at the algorithm level.
Algorithms that are differentially private inject noise at the input, model or output level to ensure that the output distribution of the model does not change substantially with the inclusion or exclusion of a data point.

\section{Sustainability}

The reality of any e-commerce company is that increase revenue almost by definition means more waste  and increased carbon released into the atmosphere  as a result of production and transport.
Efforts to mitigate these unwelcome side effects can be successful at a government level, though  the author firmly believes there ought to be stronger intergovernmental regulation to tax carbon emissions and the consumption of materials, as currently there is absolutely no incentive for retailers or consumers to reduce waste, and few incentives to recycle it.
The best a data scientist working for a retailer can hope for is poor performance of his models, which would not lead to increased sales.

\section{Delimitations}

our models do not explicitly model the correlation between output variab

\section{Outline}
