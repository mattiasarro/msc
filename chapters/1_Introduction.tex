\chapter{Introduction}

% \chapterprecishere{''Begin at the beginning'', the King said gravely,'' and go on till you come to the end: then stop.'' \par\raggedleft --- \textup{Lewis Carroll}, Alice in Wonderland}

% ML is successful especially DL
Machine learning (ML) has recently become a popular research area, and is increasingly used in industrial processes and consumer products.
A lot of this newfound interest, hype, and hysteria is directed at  neural networks and deep learning.
This focus is not unfounded - deep learning approaches continue to break benchmarks in  core machine learning research areas such as computer vision [cite], speech recognition [cite], and an increasing number of natural language processing tasks [cite NMT].
Deep learning has also revolutionised reinforcement learning, achieving superhuman performance in complex games and driving vehicles in real-world situations.
There are even limited results in  beating human at highly uncertain games with various actors such as [Texas Holdem] poker [cite].

% why DL is useful and flexible
Artificial neural networks consist of layers of transformations that map multidimensional inputs to (usually multidimensional or structured) outputs.
A single-layer neural network is a linear transformation of the inputs; each additional layer with a non-linear activation function enables the network to partition the output space and hence approximate more complex functions.
Complex models are prone to overfitting and require more training data and heavier regularisation, yet deep models are somewhat unique in that their performance continues to increase with the size of the dataset, whereas the benefits of more data taper off for many other kinds of models.

Neural networks are very flexible: one can start out with a shallow model and increase model complexity (width and depth) to see the effects on performance and generalisation.
Logistic regression might well be the most appropriate approach for a classification problem, but this is often not obvious up front, so it is beneficial to build models in a framework that also supports deep models.
In addition to easy experimentation with model architectures, it is easy to do transfer learning with deep models on popular platforms: for example, image or text models pretrained on large datasets can be utilised to increase product classification performance, and representations learned for classification could in turn be used by a downstream product recommender system.
Datasets with abundant unlabeled data can benefit from unsupervised and semisupervised learning using deep generative models.

There are competing demands in any machine learning project: predictive performance, label complexity, interpretability, computational requirements and engineering challenges.
In this work we are concerned with product classification and similarity, but another project might have vastly different priorities and expectations of the above aspects.


%  balancing between predictive performance, label complexity, transfer learning capabilities
% It is not immediately obvious  which kind of model should one use for a given task and dataset.
% A data scientist can consider the following factors:  how many labelled and unlabelled data points do we have,  how much do we value predictive performance, interpretability, and whether we want to do some transfer learning or joint training with the models.
% Labelling is often expensive, so in many real world use cases a lower label complexity (number of labels needed to obtain the desired accuracy) is preferred over slightly better performance.
% Deep learning seems to have a disadvantage in this aspect, but as we see in section [ref] in cases where unlabelled data is also abundant, semi-supervised and generative models can overcome low label complexity while increasing computation time.
% In cases where the ability of neural networks to learn features that can be used in downstream models (e.g. features learned for classification could later be used as part of a recommended system) this increased computation and engineering complexity might be justifiable.


% we explore individual models, ensembling, active learning
In this thesis, we explore three orthogonal ways of efficiently learning on a proprietary dataset for product classification, where initial labels are abundant but noisy. We first evaluate different kinds of models (shallow, deep, tree-structured, convolutional, recurrent) that are trained on different modalities / input dimensions (image, text, categorical, numerical) of the same data. After determining the performance of these baseline models, the strongest models are trained as an ensemble that outperforms each individual baseline model. Finally we fine-tune the ensemble via an active learning strategy described in section [ref], where a combination of uncertainty and disagreement sampling determines a batch of products to be labeled for the next training iteration. This overcomes the noisiness and incompleteness of the initial labels without requiring much manual labeling.

\section{Problem}

% should this be in the problen statement?
The client company gathers data from various affiliate networks (that in turn give their data from various retailers) and displays the data on their online store.
There are millions of products belonging to roughly 800 categories, and categories follow the usual nested tree structure.
The incoming data is extremely noisy and inconsistent: what kind of data is stored in what kind of feature column varies across affiliate networks, across retailers within an affiliate network, and the data within a retailer can have lots of missing values, noisy text, missing images, etc.
There is currently a rule-based system for assigning products to categories: all products matching a condition (e.g. title contains the word ``trousers'') will be assigned to that category, i.e. categories are not mutually exclusive.
This way of categorising products works relatively well on some categories, but such a rule-based system has several drawbacks: these rules are cumbersome to define, their evaluation is manual, they failed to match a large fraction of products that in principle should be in a given category, it is hard to trace back the rule that caused a false positive, and such rules are limited to textual data.

The client needs a classification system of binary independent outputs to replace the old way of categorising products.
The system should be able to learn from the output of the old system, and if possible produce models that can be used in downstream tasks such as recommend systems and product similarity models (transfer learning).
The highest priority is low label complexity, beating requirements for high accuracy and transfer learning capabilities.
The system should be robust to  noisy inputs; data preprocessing should not consider the idiosyncrasies of each affiliate network.
There is an additional feature the client requires: given a product image, the visitor should be shown products that are visually similar.

\section{Purpose}

The academic purpose of this work is to (1) assess the relative strengths of different kinds of models and their ensembles, and (2) to determine whether an active labelling strategy reduces label complexity on a real-world dataset. Analogously, the commercial purpose is to (1) obtain a model with powerful predictive capabilities, and to (2) reduce costs by using an efficient labelling strategy, and (3) obtain a high-quality product similarity score.

\section{Goals}

The goals of the work, in chronological order, is to:

\begin{itemize}
  \item Use a pre-trained 2-dimensional convolutional neural network (2D CNN) to extract features for an approximate nearest-neighbour search of visually similar products.
  \item Build an interface for subjectively evaluating the visual similarity algorithm.
  \item Train  baseline model to reproduce the behaviour of the rule-based system.
  \item Build an interface for labelling products and obtain small dataset with ground truth labels (further referred to as the ``ground truth dataset'').
  \item Train a number of different models on the rule-based labels.  Evaluate these models on the rule-based test set as well as the ground truth dataset.
  \item Train a selection of models that had good performance as an ensemble, preferring model diversity over good performance. Evaluate this model on the rule-based test set as well as the ground truth dataset.
  \item Implement the active labelling mechanism defined in [ref] and fine-tune the ensemble (that is pre-trained on rule-based labels) in 10 labelling rounds.
  \item Document the results as well as the technical architecture and workflow.
\end{itemize}

\section{Hypotheses}

The following hypotheses were postulated prior to running most experiments\footnote{the Wide\&Deep baseline model had been trained on rule-based labels}:

\begin{enumerate}
  \item Pre-trained 2D CNNs perform reasonably for visual item similarity, but fine-tuning might be needed.
  \item Linear models are relatively good at predicting higher-level categories, worse at predicting lower-level categories.
  \item Deep models for histogram / tf-idf inputs do not improve substantially on linear models.
  \item Shallow models with embedding inputs generalise better to the ground truth data, and deep models with embedding inputs generalise slightly more.
  \item XGBoost is the best performing model  for textual and categorical data.
  \item Convolutional  neural networks (CNNs) that are pre-trained on Imagenet data are consistently good predictors  of clothing products, yet failed to accurately predict categories such as technology.
  \item Fine-tuning CNNs  will give little, if any,  performance  improvement due to inefficient training data.
  % \item Training CNNs with DCGANs would obtain better results than fine-tuning CNNs, yet given the strong baseline performance of pre-trained CNNs this is not crucial for a good overall performance.
  \item Ensembling  gives better performance than any individual model.
  \item A meta-learner with a few layers  obtains better results than ensembling by averaging.
  \item Active learning substantially decreases label complexity.  This will be evaluated subjectively, given the  difficulties outlined in section [ref].
\end{enumerate}

\hfill \break
How these hypotheses were evaluated is described in section \ref{evaluation}.

% \section{Work Environment}
% \section{Deployment Environment}

\section{Ethical Consideration}

Given how pervasive  machine learning is becoming across areas of society, it is good that discussions are happening about safety, transparency and bias in machine learning systems that have the ability to affect lives.
The worst case scenario for inaccurate,  opaque or biased product classification  is embarrassment for the client company,  therefore ethical considerations not of much concern for the client company.
It is still worth reminding the reader, who might use some of the ideas in this work,  that machine learning  models always contain some kind of bias:  from the data users as input (the way it is gathered, what is gathered),  the way the data is processed, and the kinds of models used.

Already at this early stage of machine learning adoption there are cases where blackbox algorithms have been used to decide decisions that severely influence persons life -  predicting reoffending probability  of convicts to determine  whether they are given parole or not - with terrible results.
The model that was touted to be and objective substitute to a subjective judge has simply learnt that biases inherent to the data (the prejudices and racism of  the judges at the time):  it consistently underestimated reoffending rate of white convicts and overestimated the reoffending rate for black convict [cite].
Even though the data did not contain explicit information about race, the algorithm  was capable of implicitly detecting grace through some other variable that was correlated with race.
Most data scientists will never build models with such severe consequences, but even more subtle decisions  made autonomously by algorithms  can prolong the  inequalities of our society by consistently favouring certain  characteristics such as race, gender, place of origin, etc.
Prime examples are algorithms that determine whether one gets a mortgage or not, what the premium on their insurance would be, whose CV gets shortlisted for a position, and so on.

\section{Sustainability}

The reality of any e-commerce company is that increase revenue almost by definition means more waste  and increased carbon released into the atmosphere  as a result of production and transport.
Efforts to mitigate these unwelcome side effects can be successful at a government level, though  the author firmly believes there ought to be stronger intergovernmental regulation to tax carbon emissions and the consumption of materials, as currently there is absolutely no incentive for retailers or consumers to reduce waste, and few incentives to recycle it.
The best a data scientist working for a retailer can hope for is poor performance of his models, which would not lead to increased sales.

\section{Delimitations}

our models do not explicitly model the correlation between output variab

\section{Outline}
