coding todo
    save hparams as JSON
    preprocess: don't precompute img features
    AF: pipeline without update
    web: was the product positively labeled by old system?
    if it's in a child, it's in the parent

experiments
    train on rule-based labels
        correcting for label imbalance6
            maybe not necessary to do, because imbalance might arise from sparse labels
        baseline models
            random: based on class frequencies
            text
                both linear and deep, with / without text colors
                    normalized histogram
                    tfidf
                    embeddings
                    embeddings weighted by tfidf
                deep learning
                    CNN of text: predict class
                    LSTM of text: predict class
                other
                    xgboost
                    random forest: requires pre-processing for missing values [not used]
            image
                pretrained imagenet
                pretrained imagenet + finetune
        ensembling
            weights between ensemble components
            meta learner (logistic regression)
        joint training
            deep & wide & convolutional:
                deep: embeddings of text
                wide: tfidf
                conv: image
            recurrent & wide & convolutional:
                recurrent: precomputed text embeddings from RNNs + DNN
                wide: tfidf
                conv: image
    pick the best performing model from the above
        interpret contribution of ensemble parts
    fine-tune pre-trained model with active learning
        ask users to label
            decide we need to label n=10 * 1000 products per iteration
            ask labels proportionally to a class's error rate
                frac = gives the fraction of products to label in that class
                    (1-F1_SCORE(class)) / sum of all class's F1 scores
                    could consider giving preference to lower-level categories, i.e.
                        1 - (x * 1/y), x between 0 and 1, y between 1 and 5
                        1 - (x * 1/log(y)), x between 0 and 1, y between 1 and 5
                        1 - sigmoid((x * 1/log(y))), x between 0 and 1, y between 1 and 5
                            looks good, most smooth version
                            sigmoid might not be needed as we normalize anyway
                        but it might happen automatically that higher-level cats
                        will require fewer labels as we continue training
                labels_per_class = frac * n

            users see a page of products per that has products from different sampling strategies
                random (50%)
                vis/text disagreement (25%)
                    abs(P_vis(class) - P_text(class))
                uncertainty (25%)
                    candidates: sample all products with P(entropy)
                    take a random sample of size labels_per_class from candidates

            quantify how much each sampling strategy gets surprising values
                number of products that got a "surprise" label
                    P(class) was on the other side of thres than label
                xentropy of model's previous predictions and new labels
                    would it favour one or the other type of sampling?
                combined
                    sum of xentropy of products that got a surprise value
                    quantifies how much the model would change compared to random
                    might help choose between merging these two
                        if mult gives higher score for how much a model would change, use mult, else use avg

                if there is a clear winner
                    use that sampling strategy
                else
                    combine disagreement and uncertainty (mult or avg)
                    can be done separately for each following AL train iteration
        when retraining, run these experiments in parallel
            training a model on
                model-R: randomly sampled samples
                model-A: combined disagreement & uncertainty sampling

                model-IWAL: uncertainty sampled samples with importance weighting
                    can't be reasonably implemented, because
                        when a leaf gets a label, all ancestors get a label,
                            i.e. hard to know what's the correct weighting
                        when initial prediction is bad we may want to allow
                            manually mass-assigning new labels
                        training could get unstable because classes would have variable weights
                        couldn't be combined with class-imbalance work
                    hard to tell
                        importance weight changes with the model's prediction?
                        importance weight stays the same as it was when it was predicted
                    xntropy already kind of takes this into account
            each re-labeling iteration gives us a metric of how well each model did
                random sample labels give TP/FP of both models
                from each model's prediction on the previous iteration

active learning sampling strategies
    how we chose the strategy
        trained a naive wide & deep model
            key errors reasons (see below)
            unsure of the contributions of mixtures

        see if there is much disagreement between ensemble components
            if so, use disagreement-based sampling
        there could be many reasons for having poor results
            image looks a bit like sth else (solution: disagreement sampling)
            a misleading keyword puts it in the wrong class (solution: disagreement sampling)
            the initial dataset was noisy
                many FNs: learns a low bias, most products aren't in most classes
                    solution: correcting for class imbalance?
                    solution: try to find equal numbers of positive / negative labels?
                        should happen anyway with uncertainty sampling?
                some FPs: correlated with the text query rules
                    solution: disagreement sampling

architecture
    tuning ensemble components
        run each component independently to ensure it works (AF train_pred)
        test set: small set of ground truth data (inputs + labels)
        AF pipeline: hypertune
            create dirs
            dump everything as dev set
            dump test set
            schedule TF job per ensemble component
                hypertune, each train run
                    has its own num trials and hyperparam ranges
                    creates its own train / valid split
                at the end of each model train
                    eval on test set, insert metrics to ES
                    eval on valid set, insert metrics to ES
        AF pipeline: active learning retrain
            create dirs
            dump dev / test sets
            copy ground truth labels
            schedule TF job per ensemble component
                creates its own train / valid split
                at the end
                    eval on test set, insert metrics to ES
                    eval on valid set, insert metrics to ES
            schedule meta-learner TF job
                create dir
                compose dataset:
                    read each components preds into dataset
                    zip datasets together
                    shuffle&batch
                train, predict
                    eval on test set, insert metrics to ES
                    eval on valid set, insert metrics to ES
                determine next to-be labels
