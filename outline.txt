plan
    decide on AL sampling and weighting scheme
    decide on fine-tune system architecture
    research class imbalance correction

problems
    high label complexity
    long train times
    unbalanced data
    ensembling vs joint training
    interpretability of the ensemble
    combining deterministic system with manual labels + active learning
experiments
    train on rule-based labels
        experiment with correcting for label imbalance
        baseline models
            random: based on class frequencies
            text
                both linear and deep
                    normalized histogram
                    tfidf
                    embeddings
                    embeddings weighted by tfidf
                deep learning
                    CNN of text: predict class
                    LSTM of text: predict class
            image
                pretrained imagenet
                pretrained imagenet + finetune
        ensembling
            weights between ensemble components
            meta learner (logistic regression)
        joint training
            deep & wide & convolutional:
                deep: embeddings of text
                wide: tfidf
                conv: image
            recurrent & wide & convolutional:
                recurrent: precomputed text embeddings from RNNs + DNN
                wide: tfidf
                conv: image
    pick the best performing model from the above
        interpret contribution of ensemble parts
    fine-tune pre-trained model with active learning
        ask users to label
            decide we need to label n products per iteration
            ask labels proportionally to a class's error rate
                per-class softmax( F1_SCORE)
                frac = gives the fraction of products to label in that class
                labels_per_class = frac * n

            users see a page of products per that has products from different sampling strategies
                random
                vis/text disagreement: abs(P_vis(class) - P_text(class))
                uncertainty
                    candidates: sample all products with P(entropy)
                    take a random sample of size labels_per_class from candidates
        when retraining, run these experiments in parallel
            training a model on
                model-R: randomly sampled samples
                model-U: uncertainty sampled samples
                model-IWAL: uncertainty sampled samples with importance weighting
                    importance weight changes with the model's prediction?
                    importance weight stays the same as it was when it was predicted
            each re-labeling iteration
                gives us a metric of how well the models did
                random sample labels give TP/FP of all models
                    from each model's prediction on the previous iteration

theory
    data-efficient learning
        ensembling
            bagging
            boosting
        active learning
            retraining-based
            retraining-free
            sampling strategies
                compensating for sampling bias
            for deep learning
                not as common, maybe because it requires an explicit list of hypotheses
                    intractable for DL but cohn 1994 is one approach
    interpretability of models
        weights of ensemble parts / meta-learner coefficients
        CNN visualizations
        RNN hidden state analysis


active learning sampling strategies
    query by committee: label items with biggest disagreement between committee members
        could use our ensemble components as committee
        For measuring the level of disagreement, two main approaches have been proposed.
            The first is vote entropy (Dagan and Engelson, 1995):
            Another disagreement measure that has been proposed is average Kullback-Leibler (KL) divergence (McCallum and Nigam 1998):
    uncertainty sampling
        maximum  entropy  of  the  estimated  label  [5]
        minimum distance from the decision boundary [6,7]
    expected error reduction [8]:
    variance reduction [2]
    min-max

    how we chose the strategy
        trained a naive wide & deep model
            key errors reasons (see below)
            unsure of the contributions of mixtures

        see if there is much disagreement between ensemble components
            if so, use disagreement-based sampling
        there could be many reasons for having poor results
            image looks a bit like sth else (solution: disagreement sampling)
            a misleading keyword puts it in the wrong class (solution: disagreement sampling)
            the initial dataset was noisy
                many FNs: learns a low bias, most products aren't in most classes
                    solution: correcting for class imbalance?
                    solution: try to find equal numbers of positive / negative labels?
                        should happen anyway with uncertainty sampling?
                some FPs: correlated with the text query rules
                    solution: disagreement sampling

architecture


quotes
    ...
        It is important that the selection probabilities are always greater than
        zero. is guarantees that the algorithm will eventually converge to the
        optimal hypothesis and does not show the missed cluster problem. e
        missed cluster problem (Dasgupta and Hsu, 2008) explains how simple
        active learners can end up in a local optimum and can produce a classiĕer
        that is very far from the optimal classiĕer.
    cohn 1994
        Since an entire neural network configuration represents a single concept, a complete version
        space cannot be directly represented by any single neural network. In fact, Haussler
        (1987) pointed out that the size of the S and G sets could grow exponentially in the size
        of the training set. Representing these sets completely would require keeping track of and
        manipulating an exponential number of network configurations.
    Active Learning Using Uncertainty Information
        Query-by-committee put forward multiple mod-els  as  the  committees  and  selected  the  samples  which  receivehighest  level  of  disagreement  from  the  committees  [4].
        2 types of AL:
            retraining-based (retrain model after labelling each data point)
            retraining-free (uncertainty sampling, query by committee)
    Improving Generalization with Active Learning* (Cohn 1994)
        learn a general and specific NN
        use the disagreement between them as a measure of likelihood of selecting a data point
