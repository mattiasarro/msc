plan
    decide on AL sampling and weighting scheme
    decide on fine-tune system architecture
    coding todo
        web: was the product positively labeled by old system?
        if it's in a child, it's in the parent

problems
    high label complexity
    long train times
    unbalanced data
    ensembling vs joint training
    interpretability of the ensemble
    combining deterministic system with manual labels + active learning
experiments
    train on rule-based labels
        correcting for label imbalance
            maybe not necessary to do, because imbalance might arise from sparse labels
        baseline models
            random: based on class frequencies
            text
                both linear and deep
                    normalized histogram
                    tfidf
                    embeddings
                    embeddings weighted by tfidf
                deep learning
                    CNN of text: predict class
                    LSTM of text: predict class
                other
                    xgboost
                    random forest: requires pre-processing for missing values [not used]
            image
                pretrained imagenet
                pretrained imagenet + finetune
        ensembling
            weights between ensemble components
            meta learner (logistic regression)
        joint training
            deep & wide & convolutional:
                deep: embeddings of text
                wide: tfidf
                conv: image
            recurrent & wide & convolutional:
                recurrent: precomputed text embeddings from RNNs + DNN
                wide: tfidf
                conv: image
    pick the best performing model from the above
        interpret contribution of ensemble parts
    fine-tune pre-trained model with active learning
        ask users to label
            decide we need to label n=10 * 1000 products per iteration
            ask labels proportionally to a class's error rate
                frac = gives the fraction of products to label in that class
                    (1-F1_SCORE(class)) / sum of all class's F1 scores
                    could consider giving preference to lower-level categories, i.e.
                        1 - (x * 1/y), x between 0 and 1, y between 1 and 5
                        1 - (x * 1/log(y)), x between 0 and 1, y between 1 and 5
                        1 - sigmoid((x * 1/log(y))), x between 0 and 1, y between 1 and 5
                            looks good, most smooth version
                            sigmoid might not be needed as we normalize anyway
                        but it might happen automatically that higher-level cats
                        will require fewer labels as we continue training
                labels_per_class = frac * n

            users see a page of products per that has products from different sampling strategies
                random (50%)
                vis/text disagreement (25%)
                    abs(P_vis(class) - P_text(class))
                uncertainty (25%)
                    candidates: sample all products with P(entropy)
                    take a random sample of size labels_per_class from candidates

            quantify how much each sampling strategy gets surprising values
                number of products that got a "surprise" label
                    P(class) was on the other side of thres than label
                xentropy of model's previous predictions and new labels
                    would it favour one or the other type of sampling?
                combined
                    sum of xentropy of products that got a surprise value
                    quantifies how much the model would change compared to random
                    might help choose between merging these two
                        if mult gives higher score for how much a model would change, use mult, else use avg

                if there is a clear winner
                    use that sampling strategy
                else
                    combine disagreement and uncertainty (mult or avg)
                    can be done separately for each following AL train iteration
        when retraining, run these experiments in parallel
            training a model on
                model-R: randomly sampled samples
                model-A: combined disagreement & uncertainty sampling

                model-IWAL: uncertainty sampled samples with importance weighting
                    can't be reasonably implemented, because
                        when a leaf gets a label, all ancestors get a label,
                            i.e. hard to know what's the correct weighting
                        when initial prediction is bad we may want to allow
                            manually mass-assigning new labels
                        training could get unstable because classes would have variable weights
                        couldn't be combined with class-imbalance work
                    hard to tell
                        importance weight changes with the model's prediction?
                        importance weight stays the same as it was when it was predicted
                    xntropy already kind of takes this into account
            each re-labeling iteration gives us a metric of how well each model did
                random sample labels give TP/FP of both models
                from each model's prediction on the previous iteration

theory
    data-efficient learning
        ensembling
            bagging
            boosting
        active learning
            retraining-based
            retraining-free
            sampling strategies
                compensating for sampling bias
            for deep learning
                not as common, maybe because it requires an explicit list of hypotheses
                    intractable for DL but cohn 1994 is one approach
    interpretability of models
        weights of ensemble parts / meta-learner coefficients
        CNN visualizations
        RNN hidden state analysis


active learning sampling strategies
    query by committee: label items with biggest disagreement between committee members
        could use our ensemble components as committee
        For measuring the level of disagreement, two main approaches have been proposed.
            The first is vote entropy (Dagan and Engelson, 1995):
            Another disagreement measure that has been proposed is average Kullback-Leibler (KL) divergence (McCallum and Nigam 1998):
    uncertainty sampling (review paper)
        maximum  entropy  of  the  estimated  label  [5]
        minimum distance from the decision boundary [6,7]
    expected error reduction [8]:
    variance reduction [2]
    min-max

    how we chose the strategy
        trained a naive wide & deep model
            key errors reasons (see below)
            unsure of the contributions of mixtures

        see if there is much disagreement between ensemble components
            if so, use disagreement-based sampling
        there could be many reasons for having poor results
            image looks a bit like sth else (solution: disagreement sampling)
            a misleading keyword puts it in the wrong class (solution: disagreement sampling)
            the initial dataset was noisy
                many FNs: learns a low bias, most products aren't in most classes
                    solution: correcting for class imbalance?
                    solution: try to find equal numbers of positive / negative labels?
                        should happen anyway with uncertainty sampling?
                some FPs: correlated with the text query rules
                    solution: disagreement sampling

architecture


quotes
    ...
        It is important that the selection probabilities are always greater than
        zero. is guarantees that the algorithm will eventually converge to the
        optimal hypothesis and does not show the missed cluster problem. e
        missed cluster problem (Dasgupta and Hsu, 2008) explains how simple
        active learners can end up in a local optimum and can produce a classiĕer
        that is very far from the optimal classiĕer.
    cohn 1994
        Since an entire neural network configuration represents a single concept, a complete version
        space cannot be directly represented by any single neural network. In fact, Haussler
        (1987) pointed out that the size of the S and G sets could grow exponentially in the size
        of the training set. Representing these sets completely would require keeping track of and
        manipulating an exponential number of network configurations.
    Active Learning Using Uncertainty Information
        Query-by-committee put forward multiple mod-els  as  the  committees  and  selected  the  samples  which  receivehighest  level  of  disagreement  from  the  committees  [4].
        2 types of AL:
            retraining-based (retrain model after labelling each data point)
            retraining-free (uncertainty sampling, query by committee)
    Improving Generalization with Active Learning* (Cohn 1994)
        learn a general and specific NN
        use the disagreement between them as a measure of likelihood of selecting a data point
